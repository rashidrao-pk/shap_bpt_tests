{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed99a3ee",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Running cell below will import the required libraries being used in the rest of the cells of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc20645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pickle \n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, importlib\n",
    "from matplotlib import rc\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import gaussian\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from pynvml import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from datetime import datetime\n",
    "\n",
    "rc('text',usetex=True)\n",
    "rc('text.latex', preamble='\\\\usepackage{color}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "logging.getLogger('matplotlib').setLevel(level=logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import shap_bpt as shap_bpt\n",
    "import chime\n",
    "chime.theme('chime')\n",
    "\n",
    "print('shap_bpt version :', shap_bpt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6013d6-a87a-4d73-91f9-f2ed162252bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Input Tensor 0 did not already require gradients, required_grads has been set automatically\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee94551",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# import torchvision\n",
    "use_mps = ('mps' in dir(torch.backends)) and torch.backends.mps.is_available()\n",
    "torch.manual_seed(12345)\n",
    "device      = None\n",
    "\n",
    "if   use_cuda:    device = torch.device(\"cuda\")\n",
    "elif use_mps:     device = torch.device(\"mps\")\n",
    "else:             device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Torch CUDA\\t :\\t {torch.cuda.is_available()}')\n",
    "print(f'device\\t\\t   :\\t {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697493d4-0410-4548-955f-73f9272ce08b",
   "metadata": {},
   "source": [
    "### DEFAULT PARAMS\n",
    "Below cell contains the parameters switching between the prediction model, background replacement strategy, and other few parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ae750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_no = 'E1'\n",
    "exp_no = 'E1_ViT'\n",
    "# exp_no = 'E2'\n",
    "# exp_no = 'E3'\n",
    "\n",
    "USE_SOFTMAX= False\n",
    "\n",
    "model_softmax = 'softmax' if USE_SOFTMAX else 'logits'\n",
    "\n",
    "print(f'using exp: {exp_no } and model_softmax {model_softmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    dummy                   : bool  = False\n",
    "    exp_no                  : str   = exp_no          \n",
    "    model_type              : str   = ''\n",
    "    pretrained_model_type   : str   = ''\n",
    "    background_type         : str   = ''\n",
    "params = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_from_exp(params):\n",
    "    if params.exp_no=='E1':\n",
    "        params.model_type              = 'real'\n",
    "        params.pretrained_model_type   = 'resnet'      \n",
    "        params.background_type         = 'gray'\n",
    "    if params.exp_no=='E1_ViT':\n",
    "        params.model_type              = 'real'\n",
    "        params.pretrained_model_type   = 'vit'      \n",
    "        params.background_type         = 'gray'\n",
    "    elif params.exp_no=='E2':\n",
    "        params.model_type              = 'ideal'\n",
    "        params.pretrained_model_type   = 'resnet'      \n",
    "        params.background_type         = 'gray'\n",
    "        params.file_name               = 'exp_E2'\n",
    "    elif params.exp_no=='E3':\n",
    "        params.model_type              = 'real'\n",
    "        params.pretrained_model_type   = 'swin_trans_vit'      \n",
    "        params.background_type         = 'gray'\n",
    "    return params\n",
    "\n",
    "params = get_params_from_exp(params)\n",
    "params.file_name = f'exp_{params.exp_no}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55482e05-c3c5-41bc-9ba4-05b450b6fde1",
   "metadata": {},
   "source": [
    "## Select model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaf133-6696-4764-8378-d8a88b480f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type       = 'real'   # Exp 1   \n",
    "# model_type       = 'ideal'    # Exp 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb36ce-0206-47f3-9667-f82a256519a3",
   "metadata": {},
   "source": [
    "## Select pretrained_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba60f24-24dd-48e0-bdf7-60f7713aced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model_type           = 'resnet'                #   Exp 1,2,3\n",
    "# pretrained_model_type           = 'swin_trans_vit'      #   Exp 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bf5b4-ffb7-4b56-be4b-f5dd350eda15",
   "metadata": {},
   "source": [
    "## Select background_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cde65-9346-4dea-b2fe-2224d8ad4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_type  = 'gray'     # Exp 1 and Exp 2\n",
    "# background_type  = 'full'     # Exp 3\n",
    "###############################################################################\n",
    "# background_type  = 'black'     \n",
    "# background_type  = 'white'     \n",
    "# background_type  = 'noise'     \n",
    "# background_type  = 'blurred'     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bf329-6b39-43a1-8195-0ac9fd2a5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if params.model_type =='ideal':\n",
    "#     params.file_name = 'exp_E2'\n",
    "# elif params.model_type =='real':\n",
    "#     if params.pretrained_model_type=='resnet' and params.background_type=='gray':\n",
    "#         params.file_name = 'exp_E1'\n",
    "#     if params.pretrained_model_type=='swin_trans_vit':\n",
    "#         params.file_name = 'exp_E5'\n",
    "#     if params.background_type=='full':\n",
    "#         params.file_name = 'exp_E3'\n",
    "# if random_bpt:\n",
    "#     file_name = 'exp_E3'\n",
    "\n",
    "def get_exp_header():\n",
    "    print('='*95)\n",
    "    print('| ',f'{\"MODEL_TYPE\":15}','|',f'{\"PRETRAINED_MODEL\":22}','|',f'{\"BACKGROUND_TYPE\":22}','|',f'{\"FILENAME\":22}','|')\n",
    "    print('-'*95)\n",
    "    print('| ',f'{params.model_type:15}','|',f'{params.pretrained_model_type:22}','|',f'{params.background_type:22}','|',f'{params.file_name:22}','|')\n",
    "\n",
    "get_exp_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d16f2b-544f-4e7f-be2e-6251a85788fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_explained_classes  =  1         # No of Top Predicted Classes\n",
    "max_weight             =  None      # Hyperparameter for BPT and Axis-Aligned \n",
    "\n",
    "model_version          = 'v2'       # v1 , v2\n",
    "# rel_pool_type          = 'sum_pos'  # required for -> relevance_mass_accuracy,relevance_rank_accuracy\n",
    "suffix_full            = f'{params.model_type}_{params.background_type}'\n",
    "batch_size             =  4 if params.pretrained_model_type  == 'swin_trans_vit' else 8 # Batch Size (depending on system resources), 2 or 4 If background_type=full "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f95a58-0eb2-4261-842c-120fc58e6870",
   "metadata": {},
   "source": [
    "### Paths Setting\n",
    "This cell contains the setting of paths being used to load dataset, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e016e-0843-48ba-9c0a-f2995fb7fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_version    =  False\n",
    "\n",
    "global path_seg_maps_raw,path_img_val,DS_name\n",
    "path_codes    = os.getcwd()\n",
    "path_notebook = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "DS_name_annotation    = 'ImageNetS'\n",
    "DS_name_img           = 'ImageNet'\n",
    "\n",
    "# path_ds_main   = f'{path_notebook}/dataset'   if submission_version else 'D:\\DS\\ImageNet'\n",
    "path_ds_main   = 'D:\\DS\\ImageNet'\n",
    "\n",
    "if not submission_version:\n",
    "    path_repos      = 'E:/PHD/datacloud_data/repos' \n",
    "    codes_local     = os.getcwd()\n",
    "    \n",
    "if submission_version:\n",
    "    path_repos     = os.path.join(os.getcwd(),'utils')\n",
    "    codes_local    = path_codes\n",
    "sys.path.append(path_repos)\n",
    "submission_version    =  True\n",
    "\n",
    "path_utils    = os.path.abspath(os.path.join(path_notebook,'utils'))\n",
    "\n",
    "path_seg_maps  = os.path.join(path_ds_main,f'{DS_name_annotation}/ILSVRC2012_img_val_S/ImageNetS50/validation-segmentation')\n",
    "path_img_val   = os.path.join(path_ds_main,f'{DS_name_img}/ILSVRC2012_img_val')\n",
    "\n",
    "filenames,files = [],[]\n",
    "tagnames      = os.listdir(path_seg_maps)\n",
    "for tn in tagnames:\n",
    "    sub_dir = f'{path_seg_maps}//{tn}'\n",
    "    filenames.extend(os.listdir(sub_dir))\n",
    "    tempp = []\n",
    "    for fn in os.listdir(sub_dir):\n",
    "        tempp.append(f'{sub_dir}//{fn}')\n",
    "    files.extend(tempp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e103fd-9957-4142-b699-8ef35063b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix                     = f'{params.pretrained_model_type}_{params.model_type}_{params.background_type}'\n",
    "\n",
    "if submission_version:\n",
    "    results_                   = os.path.join(os.getcwd(),'results')          # for submission\n",
    "else:\n",
    "    results_                   = os.path.join(codes_local,'results')\n",
    "    \n",
    "results_cloud                  = os.path.join(path_codes,'results')\n",
    "results_path                   = os.path.join(results_,params.pretrained_model_type)\n",
    "\n",
    "results_path                   = os.path.join(results_path,suffix)\n",
    "results_path_single            = os.path.join(results_path      ,'single')\n",
    "results_path_selected          = os.path.join(results_path      ,'selected')\n",
    "plots_path                     = os.path.join(results_path,'plots')\n",
    "plotsIoU_path                  = os.path.join(results_path,'plots_IoU')\n",
    "vector_path                    = os.path.join(results_path,'vectors')\n",
    "path_csv                       = os.path.join(results_,     'csv_logits')\n",
    "results_path_fixed             = results_path\n",
    "\n",
    "os.makedirs(results_path,              exist_ok=True)\n",
    "os.makedirs(plots_path,                exist_ok=True)\n",
    "os.makedirs(plotsIoU_path,             exist_ok=True)\n",
    "os.makedirs(results_path_single,       exist_ok=True)\n",
    "os.makedirs(results_path_selected,     exist_ok=True)\n",
    "os.makedirs(vector_path,               exist_ok=True)\n",
    "\n",
    "print('results_path\\t\\t',results_path)\n",
    "print('results_path_single\\t',results_path_single)\n",
    "print('results_path_selected\\t',results_path_selected)\n",
    "print('-'*120)\n",
    "print('Dataset path \\t\\t',path_ds_main)\n",
    "print('-'*120)\n",
    "\n",
    "print('path_csv\\t\\t',path_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b1235-e73f-421c-b1d9-8f8d73ccdc95",
   "metadata": {},
   "source": [
    "## BlackBox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "sys.path.append(f'{path_repos}/Transformer-Explainability')\n",
    "from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
    "\n",
    "model = None\n",
    "def load_model():\n",
    "    global model,model_version\n",
    "    print(f'Model loaded\\t:{params.pretrained_model_type}')\n",
    "    # ------------- resnet \n",
    "    if params.pretrained_model_type=='resnet':\n",
    "        model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1 if model_version == 'v1' else ResNet50_Weights.IMAGENET1K_V2)\n",
    "    ######################################################################################################\n",
    "    # ------------- VIT \n",
    "    elif params.pretrained_model_type   == 'vit':\n",
    "        # pass\n",
    "        from torchvision.models import vit_b_16\n",
    "        model = vit_b_16(weights='IMAGENET1K_V1')\n",
    "        model = model.to(device)\n",
    "    ######################################################################################################\n",
    "    # ------------- vgg16 \n",
    "    elif params.pretrained_model_type=='vgg16':\n",
    "        model = models.vgg16(pretrained=True)\n",
    "    ######################################################################################################\n",
    "    # ------------- swin_trans_vit   \n",
    "    elif params.pretrained_model_type=='swin_trans_vit':\n",
    "\n",
    "        model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "    ######################################################################################################\n",
    "    # ------------- vit_LRP \n",
    "    elif params.pretrained_model_type=='vit_LRP':\n",
    "        model = vit_LRP(pretrained=True)\n",
    "    # ------------- vit_base \n",
    "    elif params.pretrained_model_type=='vit_base':\n",
    "        model = timm.create_model('vit_base_patch16_224', pretrained=True).to(device)\n",
    "    # ------------- VIT \n",
    "    # elif params.pretrained_model_type =='':\n",
    "\n",
    "        # from timm.models import swin_base_patch4_window7_224_in22k\n",
    "        # from pytorch_grad_cam import GradCAMPlusPlus --> https://github.com/jacobgil/pytorch-grad-cam/issues/84\n",
    "    \n",
    "    model = model.to(device).eval()\n",
    "\n",
    "\n",
    "##########################################################\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "resnet50_preprocess = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "load_model() # resnet         vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea84914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# Forward function with mixed precision\n",
    "# scaler = GradScaler()\n",
    "\n",
    "if USE_SOFTMAX:\n",
    "    print('Using Softmax')\n",
    "    def f(x):\n",
    "        ################### CHANGES\n",
    "        # with autocast():\n",
    "        return F.softmax(model(x), dim=1).cpu().detach().numpy()\n",
    "else:\n",
    "    print('NO Softmax')\n",
    "    def f(x):\n",
    "    ################### CHANGES\n",
    "    # with autocast():\n",
    "        return model(x).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_utils}/imagenet_class_index.json') as file:\n",
    "    CLS2IDX = json.load(file)\n",
    "with open(f'{path_utils}/imagenet_class_index.json') as file:\n",
    "    jf = json.load(file).values()\n",
    "    class_names = [v[1] for v in jf]\n",
    "    tag_to_classid = { v[0]:i for i,v in enumerate(jf) }\n",
    "\n",
    "imagenetS_to_imagenet = {}\n",
    "with open(f'{path_utils}/ImageNetS_categories_im50.txt') as file:\n",
    "    for i,line in enumerate(file):\n",
    "        imagenetS_to_imagenet[i+1] = tag_to_classid[line.strip()]\n",
    "classid_to_tag= dict(zip(tag_to_classid.values(),tag_to_classid.keys()))\n",
    "\n",
    "image_to_explain              = None\n",
    "image_to_explain_preproc      = None\n",
    "image_to_explain_tensor       = None\n",
    "ground_truth                  = None\n",
    "\n",
    "background_tensors            = None\n",
    "background_image_set          = None\n",
    "background_image_preproc_set  = None\n",
    "weighted_ground_truth         = None\n",
    "predicted_fS = sorted_classes = predicted_cls = predicted_f0 = f_S = f_0 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8533d7-8bb9-482a-9044-8e8a16f98cd3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a4500",
   "metadata": {},
   "source": [
    "### LOAD IMAGE AND CONVERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(fname,loaded_image=False,bg_type=None):\n",
    "    global path_img_val\n",
    "    global image_to_explain, image_to_explain_preproc, image_to_explain_tensor\n",
    "    # global predicted_fS, predicted_f0, predicted_cls, sorted_classes, f_S, f_0\n",
    "    global ground_truth,weighted_ground_truth\n",
    "    global model_type\n",
    "    global background_tensors, background_image_set, background_image_preproc_set\n",
    "    ground_truth = cv2.imread(f'{fname}', cv2.IMREAD_COLOR)[:,:,::-1]\n",
    "    ground_truth = cv2.resize(ground_truth, [224,224], interpolation=cv2.INTER_NEAREST)\n",
    "    ground_truth = ground_truth[:,:,0].astype(int) + 256*ground_truth[:,:,1].astype(int)\n",
    "    ground_truth[ ground_truth==1000 ] = 0\n",
    "    \n",
    "    weighted_ground_truth = gaussian_filter(ground_truth.astype(float), 16) * ground_truth\n",
    "    file_n = fname.split('//')[-1].split('.')[0]\n",
    "    if not loaded_image:\n",
    "        # Foreground image to be explained\n",
    "        image_to_explain         = cv2.resize(cv2.imread(f'{path_img_val}/{file_n}.JPEG', cv2.IMREAD_COLOR),[224,224])[:,:,::-1]\n",
    "    \n",
    "    if params.model_type=='ideal':\n",
    "        image_to_explain_preproc  = torch.ones(tuple(reversed(image_to_explain.shape)))\n",
    "    else:\n",
    "        image_to_explain_preproc  = resnet50_preprocess(image_to_explain.astype(np.float32)/255.0)\n",
    "        \n",
    "    image_to_explain_tensor = image_to_explain_preproc.to(device)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    bkgnd0 = np.full_like(image_to_explain, 0)\n",
    "    bkgnd1 = np.full_like(image_to_explain, 127)\n",
    "    bkgnd2 = np.full_like(image_to_explain, 255)\n",
    "    bkgnd3 = gaussian(image_to_explain, 8, channel_axis=-1)*255\n",
    "    bkgnd4 = np.clip(np.random.normal(128, 128, size=image_to_explain.shape), 0, 255).astype(np.uint8)\n",
    "    bkgnd4 = (gaussian(bkgnd4, 2.0, channel_axis=-1) * 255).astype(np.uint8)\n",
    "    if bg_type=='black':\n",
    "        background_image_set = np.array([bkgnd0])\n",
    "    elif bg_type=='gray':\n",
    "        background_image_set = np.array([bkgnd1])\n",
    "    elif bg_type=='white':\n",
    "        background_image_set = np.array([bkgnd2])\n",
    "    elif bg_type=='blurred':\n",
    "        background_image_set = np.array([bkgnd3])\n",
    "    elif bg_type=='noise':\n",
    "        background_image_set = np.array([bkgnd4])\n",
    "    elif bg_type=='full':\n",
    "        background_image_set = np.array([bkgnd0, bkgnd1, bkgnd2, bkgnd3, bkgnd4])\n",
    "    if params.model_type=='ideal':\n",
    "        background_image_preproc_set = [torch.zeros(tuple(reversed(bkgnd.shape)))\n",
    "                                        for bkgnd in background_image_set]\n",
    "    else:\n",
    "        background_image_preproc_set = [resnet50_preprocess(bkgnd.astype(np.float32)/255.0)\n",
    "                                        for bkgnd in background_image_set]\n",
    "\n",
    "    background_tensors = torch.cat([torch.unsqueeze(bk_p, dim=0) \n",
    "                                    for bk_p in background_image_preproc_set]).to(device)\n",
    "\n",
    "load_image(files[0], bg_type=params.background_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e8543-2a7e-44fe-bcb4-116e938c5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "def print_gpu_memory():\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    print(f'total    : {info.total}')\n",
    "    print(f'free     : {info.free}')\n",
    "    print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec0eff-5bf6-4488-87c3-c2eaa4be770b",
   "metadata": {},
   "source": [
    "### Masking Fucntion\n",
    "Two Masking Function being used to generate Shapely values\n",
    "- f_masked_resnet (pretrained model used for  Experiments E1)\n",
    "- f_masked_ideal  (linear model used for Experiments E2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_masked_resnet(masks):\n",
    "    global f, background_tensors, image_to_explain_tensor\n",
    "    N,H,W = masks.shape\n",
    "    B = background_tensors.shape[0]\n",
    "    masks_tensor = torch.tensor(np.array(masks)).to(device)                     # N boolean masks with size W*H\n",
    "    masks_tensor = torch.reshape(masks_tensor, (N,1,H,W))                       # N*H*W    -> N*1*H*W\n",
    "    masks_tensor = torch.tile(masks_tensor, dims=(1,3,B,1)).reshape(B*N,3,H,W)  # N*1*H*W  -> NB*3*H*W\n",
    "    Xf = torch.tile(image_to_explain_tensor, dims=(N*B,1,1,1))                  # 3*H*W    -> NB*3*H*W\n",
    "    Xb = torch.tile(background_tensors, dims=(N,1,1,1))                         # 3*H*W    -> NB*3*H*W\n",
    "\n",
    "    X = torch.where(masks_tensor, Xf, Xb) # T=Xf, F=Xb\n",
    "    result = f(X)\n",
    "    del X, Xb, Xf, masks_tensor\n",
    "    result = result.reshape((-1, B, result.shape[1]))\n",
    "    return np.mean(result, axis=1)\n",
    "    \n",
    "def f_masked_ideal(masks):\n",
    "    global ground_truth,weighted_ground_truth\n",
    "    res = []\n",
    "    \n",
    "    for m in masks:\n",
    "        prob = np.sum(np.multiply(m, weighted_ground_truth)) / np.sum(weighted_ground_truth)\n",
    "        # prob = 1.0 - math.exp(-prob)\n",
    "        \n",
    "        res.append(predicted_fS * prob) #np.tile(prob, reps=1000))\n",
    "    res = np.array(res)\n",
    "    return res\n",
    "def f_masked(masks):\n",
    "    global params\n",
    "    return f_masked_resnet(masks) if params.model_type=='real' else f_masked_ideal(masks)\n",
    "print(f'masking method : {f_masked.__name__}, model_type: {params.model_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bg_values(f_masked,ground_truth,predicted_cls,class_names, verbose=False):\n",
    "    global f_G, f_B\n",
    "    # evaluate the ground truth mask with the background replacement strategy for masking function\n",
    "    predicted_fG = f_masked(np.expand_dims(ground_truth, axis=0))[0]\n",
    "    f_G = float(predicted_fG[predicted_cls])\n",
    "    # print(class_names[predicted_cls], f_G, predicted_cls, f_G)\n",
    "    # print('softmax prob:', np_softmax(predicted_fG)[predicted_cls])\n",
    "\n",
    "    # evaluate the backgrounf (negative of the ground truth mask)\n",
    "    background_mask = np.logical_not(ground_truth)\n",
    "    predicted_fB = f_masked(np.expand_dims(background_mask, axis=0))[0]\n",
    "    f_B = float(predicted_fB[predicted_cls])\n",
    "    # print(class_names[predicted_cls], f_B, predicted_cls, f_B)\n",
    "    # print('softmax prob:', np_softmax(predicted_fB)[predicted_cls])\n",
    "\n",
    "    if verbose:\n",
    "        print('nu(S):  ', round(f_S, 4))\n",
    "        print('nu(G):  ', round(f_G, 4))\n",
    "        print('nu(S/G):', round(f_B, 4))\n",
    "        print('nu(0):  ', round(f_0, 4))\n",
    "    return f_G, f_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faee8b-e110-405a-8a61-e3807bfdc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_to_explain(fname=None,loaded_image = False, bg_type=None):\n",
    "    global predicted_fS, predicted_f0, predicted_cls, sorted_classes, f_S, f_0\n",
    "    global model_type,pretrained_model_type\n",
    "\n",
    "    global f_G, f_B\n",
    "    if not loaded_image:\n",
    "        load_image(fname,loaded_image=loaded_image,bg_type=bg_type)\n",
    "    \n",
    "    h,w,_ = image_to_explain.shape\n",
    "    # Foreground image to be explained   \n",
    "    predicted_fS = f(torch.unsqueeze(resnet50_preprocess(image_to_explain.astype(np.float32)/255.0).to(device), dim=0))[0]\n",
    "    sorted_classes = np.flip(np.argsort(predicted_fS))\n",
    "    predicted_cls = sorted_classes[0]\n",
    "    f_S = float(predicted_fS[predicted_cls])\n",
    "    #####################\n",
    "    predicted_f0 = [f(torch.unsqueeze(resnet50_preprocess(bkgnd.astype(np.float32)/255.0).to(device), dim=0))[0] for bkgnd in background_image_set]\n",
    "    predicted_f0 = np.mean(predicted_f0,axis=0)\n",
    "    f_0          = float(predicted_f0[predicted_cls])\n",
    "    f_G, f_B = get_bg_values(f_masked,ground_truth.astype(bool),predicted_cls,class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69360cca-c1c3-4dad-b120-f27a194ca96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(pretrained_model_type='vit') # resnet         vit\n",
    "load_image_to_explain(fname =files[0] ,bg_type='gray')\n",
    "# load_image(files[0], bg_type=background_type)\n",
    "print(class_names[predicted_cls],f_S)\n",
    "get_bg_values(f_masked,ground_truth.astype(bool),predicted_cls,class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022f173-fe09-4741-82df-a85664407cbc",
   "metadata": {},
   "source": [
    "### GT and IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a96d8-6466-41d4-af0e-53c94a05f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_groundtruth_explanation(gtruth, heatmap, threshold):\n",
    "    if gtruth.ndim == 3:\n",
    "        gt = gtruth[:,:,0]>0\n",
    "    else:\n",
    "        gt = gtruth[:,:]>0\n",
    "    ht = (heatmap >= threshold).astype(np.uint8)\n",
    "    img = np.zeros(shape=list(heatmap.shape)+[3], dtype=np.uint8)\n",
    "    img[:,:,0] = 255*(1-gt)\n",
    "    img[:,:,1] = 255*(1-ht)\n",
    "    img[:,:,2] = 255*(1-ht)\n",
    "    return img\n",
    "def calc_IoU_curve(y_true, y_pred, add_noise=True):\n",
    "    \n",
    "    assert len(y_true.shape)==1 and len(y_pred.shape)==1 # assumes y_true and y_pred to be flattened arrays\n",
    "    if add_noise:\n",
    "        rng = np.random.default_rng(12345)\n",
    "        # factor = np.mean(y_pred)/1000\n",
    "        y_pred = y_pred + rng.normal(0.0, 0.000000001, size=y_pred.shape) # 0.0000001\n",
    "    \n",
    "    yd = np.array(sorted(zip(y_pred, y_true), reverse=True))\n",
    "    X2   = np.zeros(len(y_pred))\n",
    "    IoU2 = np.zeros(len(y_pred))\n",
    "    Th   = np.zeros(len(y_pred))\n",
    "    \n",
    "    nT = np.sum(y_true)\n",
    "    nInt = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if yd[i,1]: \n",
    "            nInt += 1\n",
    "        \n",
    "        IoU2[i] = nInt / (i + nT - nInt)\n",
    "        X2[i] = i\n",
    "        Th[i] = yd[i,0]\n",
    "        \n",
    "    X2 = X2 / len(y_pred)\n",
    "    auc_IoU = 0\n",
    "    for i in range(1, len(y_pred)):\n",
    "        auc_IoU += (X2[i] - X2[i-1]) * (IoU2[i] + IoU2[i-1]) / 2.0\n",
    "    \n",
    "    best_pt = np.argmax(IoU2)\n",
    "    \n",
    "    if np.sum(y_pred) == 0:\n",
    "        return X2, np.zeros_like(X2), Th[best_pt], X2[best_pt], 0\n",
    "    else:\n",
    "        return X2, IoU2, Th[best_pt], X2[best_pt], auc_IoU\n",
    "        # return {'X':X2, 'Y':IoU2, 'max_IoU':Th[best_pt], 'x_best':X2[best_pt], 'auc_IoU':auc_IoU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5d643-aca1-45ab-989e-5b940cebe5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_IoU(shapley_values, threshold, ground_truth,verbose=False):\n",
    "    pred = shapley_values.flatten() >= threshold\n",
    "    real = ground_truth.flatten()\n",
    "    image = np.full((len(pred), 3), 1.0, dtype=np.float32)\n",
    "    if verbose:\n",
    "        print(np.sum(pred), np.sum(real))\n",
    "    image[ pred & real, : ]    = (0.0, 0.0, 0.75) # True Positives\n",
    "    image[ pred & (~real), : ] = (1.0, 0.6, 0.2)  # False Positives\n",
    "    image[ (~pred) & real, : ] = (1.0, 0.4, 1.0)  # False Negatives\n",
    "\n",
    "    return image.reshape(list(ground_truth.shape) + [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32aa7e-e8bd-4640-bffe-2a075b16c4f7",
   "metadata": {},
   "source": [
    "# Computer Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad79ae2-7676-42ea-bd3d-acab2af99666",
   "metadata": {},
   "source": [
    "## SAL 2 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_to_auc(nu, heatmap, f_S, f_0, predicted_cls, batch_size=4, method='del', num_samples=101, \n",
    "                    rule='trapezoid'):\n",
    "    assert isinstance(heatmap, np.ndarray)\n",
    "    assert len(heatmap.shape)==2 and np.issubdtype(heatmap.dtype, np.floating)\n",
    "\n",
    "    # nu_max = max(f_S, f_0)\n",
    "    # nu_min = min(f_S, f_0)\n",
    "\n",
    "    xs, ys, ms, masks, qs = [], [], [], [], []\n",
    "    for i, value in enumerate(np.linspace(start=1.0, stop=0.0, num=num_samples)):\n",
    "        if method=='del':\n",
    "            epsilon = (1 if value==0.0 else 0)\n",
    "            q = (np.quantile(heatmap, q=value) - epsilon)\n",
    "            m = heatmap <= q\n",
    "            nx = (1.0 - np.sum(m) / m.size)\n",
    "        elif method=='ins':\n",
    "            epsilon = (1 if value==1.0 else 0)\n",
    "            q = (np.quantile(heatmap, q=value) + epsilon)\n",
    "            m = heatmap >= q\n",
    "            nx = (np.sum(m) / m.size)\n",
    "        else:\n",
    "            raise Exception()\n",
    "            \n",
    "        # add a new datapoint on the curve\n",
    "        if len(xs)==0 or nx != xs[-1]: \n",
    "            assert m.dtype==bool and len(m.shape)==2\n",
    "            xs.append(nx)\n",
    "            masks.append(m)\n",
    "            ms.append(np.sum(heatmap[m]))\n",
    "            qs.append(q)\n",
    "\n",
    "        # evaluate the characteristic function\n",
    "        if len(masks) >= batch_size or (len(masks)>0 and i==(num_samples-1)):\n",
    "            y = nu(np.array(masks))[:, predicted_cls]\n",
    "            ys.extend(y)\n",
    "            masks = []\n",
    "\n",
    "    assert len(masks)==0    \n",
    "    xs, ys = np.array(xs), np.array(ys)\n",
    "    assert(len(xs) == len(ys))\n",
    "\n",
    "    # compute considering under/over shoots\n",
    "    if f_S > f_0:\n",
    "        overshoot_max = np.maximum(0, ys - f_S) # overshoot for values exceeding the maximum f(S)\n",
    "        overshoot_min = np.maximum(0, f_0 - ys) # overshoot for values below the minimum f(0)\n",
    "    else: # f(S) < f(0)\n",
    "        overshoot_max = np.maximum(0, ys - f_0) # overshoot for values exceeding the maximum f(0)\n",
    "        overshoot_min = np.maximum(0, f_S - ys) # overshoot for values below the minimum f(S)\n",
    "\n",
    "    # clip ys, no oveshoots\n",
    "    y_clipped = np.clip(ys, min(f_S, f_0), max(f_S, f_0))\n",
    "    # adjust ys with the overshoot. Clip it inside the admitted range\n",
    "    y_adjusted = np.clip(ys - 2*overshoot_max + 2*overshoot_min, min(f_S, f_0), max(f_S, f_0))\n",
    "\n",
    "    # rebase to f(0)\n",
    "    if f_S > f_0:\n",
    "        flipped = False\n",
    "        ys = ys - f_0 \n",
    "        y_clipped = y_clipped - f_0 \n",
    "        y_adjusted = y_adjusted - f_0\n",
    "    else: # f(S) < f(0)\n",
    "        flipped = True\n",
    "        ys = f_0 - ys \n",
    "        y_clipped = f_0 - y_clipped \n",
    "        y_adjusted = f_0 - y_adjusted\n",
    "\n",
    "    # rescaling\n",
    "    ys_rescaled = ys / abs(f_S - f_0)\n",
    "    y_clipped_rescaled = y_clipped / abs(f_S - f_0)\n",
    "    y_adjusted_rescaled = y_adjusted / abs(f_S - f_0)\n",
    "\n",
    "    auc, auc_r, auc_mae, auc_mse, auc_adj, auc_adjr, auc_clip, auc_clipr = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    curve_range = range(1, len(xs)) if rule=='trapezoid' else range(len(xs))\n",
    "\n",
    "    # compute the area under the curve with the midpoint Riemann sum (i.e. the trapezoidal rule)\n",
    "    for i in curve_range:\n",
    "        if rule=='trapezoid':\n",
    "            delta_x = abs(xs[i] - xs[i-1])\n",
    "            assert delta_x > 0\n",
    "            y_mid   =         0.5*(ys[i-1] + ys[i])\n",
    "            y_r_mid =         0.5*(ys_rescaled[i-1] + ys_rescaled[i])\n",
    "            err_mid = y_mid - 0.5*(ms[i-1] - ms[i])\n",
    "            y_clip_mid =       0.5*(y_clipped[i-1] + y_clipped[i])\n",
    "            y_clipr_mid =      0.5*(y_clipped_rescaled[i-1] + y_clipped_rescaled[i])\n",
    "            y_adj_mid =       0.5*(y_adjusted[i-1] + y_adjusted[i])\n",
    "            y_adjr_mid =      0.5*(y_adjusted_rescaled[i-1] + y_adjusted_rescaled[i])\n",
    "        else: # rectangles\n",
    "            delta_x = 1.0/num_samples if i==len(xs)-1 else abs(xs[i+1] - xs[i])\n",
    "            assert delta_x > 0\n",
    "            y_mid   =         ys[i]\n",
    "            y_r_mid =         ys_rescaled[i]\n",
    "            err_mid = y_mid - ms[i]\n",
    "            y_clip_mid =       y_clipped[i]\n",
    "            y_clipr_mid =      y_clipped_rescaled[i]\n",
    "            y_adj_mid =       y_adjusted[i]\n",
    "            y_adjr_mid =      y_adjusted_rescaled[i]\n",
    "\n",
    "\n",
    "        auc += abs(delta_x * y_mid) # base * height\n",
    "        auc_r += abs(delta_x * y_r_mid) # base * height\n",
    "        # auc_eff += abs(delta_x * err_mid) # base * height\n",
    "        auc_mae += abs(delta_x * err_mid) # base * height\n",
    "        auc_mse += abs(delta_x * (err_mid**2)) # base * height^2\n",
    "        auc_clip += abs(delta_x * y_clip_mid)\n",
    "        auc_clipr += abs(delta_x * y_clipr_mid)\n",
    "        auc_adj += abs(delta_x * y_adj_mid)\n",
    "        auc_adjr += abs(delta_x * y_adjr_mid)\n",
    "\n",
    "    return {'xs':xs, 'ms':ms, 'qs':qs, \n",
    "            'f_0':f_0, 'f_S':f_S, 'flipped':flipped, \n",
    "            'ys':ys, 'ysr':ys_rescaled,\n",
    "            'y_clip':y_clipped, 'y_clipr':y_clipped_rescaled, \n",
    "            'y_adj':y_adjusted, 'y_adjr':y_adjusted_rescaled, \n",
    "            'method':method, 'predicted_cls':predicted_cls,\n",
    "            'auc':auc, 'auc_r':auc_r,\n",
    "            'auc_mae':auc_mae, 'auc_mse':auc_mse, 'auc_rmse':np.sqrt(auc_mse), \n",
    "            'auc_clip':auc_clip, 'auc_clipr':auc_clipr,\n",
    "            'auc_adj':auc_adj, 'auc_adjr':auc_adjr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5e3d3-b3ce-402a-a38f-af8f1865fc65",
   "metadata": {},
   "source": [
    "# XAI Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a4f63",
   "metadata": {},
   "source": [
    "## 1. BPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpt_heatmaps(num_samples=100,verbose=False):\n",
    "    explainer = shap_bpt.Explainer(f_masked, image_to_explain, num_explained_classes=num_explained_classes, verbose=verbose)\n",
    "    shap_values_bpt = explainer.explain_instance(num_samples, method='BPT',\n",
    "                                             batch_size=batch_size, max_weight=max_weight)\n",
    "    del explainer\n",
    "    return shap_values_bpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137888d",
   "metadata": {},
   "source": [
    "## 2. Axis-Aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aa_heatmaps(num_samples=100,verbose=False):\n",
    "    explainer = shap_bpt.Explainer(f_masked, image_to_explain, num_explained_classes=num_explained_classes, verbose=verbose)\n",
    "    shap_values_aa = explainer.explain_instance(num_samples, method='AA', verbose_plot=False, \n",
    "                                                batch_size=batch_size, max_weight=max_weight)\n",
    "    del explainer\n",
    "    return shap_values_aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a973a-c12d-4d77-8ab9-76eb2236792f",
   "metadata": {},
   "source": [
    "## 3. SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce379dfd",
   "metadata": {},
   "source": [
    "### Partition SHAP\n",
    " - https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/partition_explainer/Partition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e0197-4b78-4b57-9556-661c45b0ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap as shap\n",
    "# shap.logging.set_verbosity(shap.logging.WARNING)\n",
    "\n",
    "def shap_predict(img):\n",
    "    assert params.model_type=='real'\n",
    "    return f(torch.Tensor(img).permute(0,3,1,2).to(device))\n",
    "\n",
    "def get_pe_heatmaps(num_samples=1000):\n",
    "    assert len(background_tensors)==1\n",
    "    shapMasker     = shap.maskers.Image(background_tensors[0].permute(1,2,0).detach().cpu().numpy(), image_to_explain.shape)\n",
    "    shapPartExpl   = shap.Explainer(shap_predict, shapMasker, output_names=class_names, algorithm=\"partition\")\n",
    "    shap_values_pe = shapPartExpl(np.expand_dims(image_to_explain_preproc.permute(1,2,0).detach().cpu().numpy(), 0), \n",
    "                                  max_evals=num_samples, batch_size=batch_size, \n",
    "                                  outputs=shap.Explanation.argsort.flip[:4],\n",
    "                                  #show_progress=False, # -- VERIFY   ??????????\n",
    "                                  )\n",
    "    shap_values_pe = np.moveaxis(np.sum(shap_values_pe.values[0], axis=2), 2, 0)\n",
    "    del shapMasker,shapPartExpl\n",
    "    return shap_values_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c15c8a-7bd5-42e2-b0e7-a07b107e5694",
   "metadata": {},
   "source": [
    "### GradientExplainer SHAP\n",
    " - https://shap-lrjball.readthedocs.io/en/latest/generated/shap.GradientExplainer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00181eb4-fd58-48de-b5b7-177b0f56aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradexpl_heatmap(use_abs=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    if params.pretrained_model_type  == 'vit_LRP':\n",
    "        return np.ones((1,224,224))\n",
    "    \n",
    "    e = shap.GradientExplainer(model, background_tensors)\n",
    "    expl = e.shap_values(torch.unsqueeze(image_to_explain_tensor, dim=0),\n",
    "                         nsamples=20, ranked_outputs=num_explained_classes)\n",
    "    heatmaps = np.sum(expl[0], axis=1)[:,:,:,0]\n",
    "    for i, clsid in enumerate(sorted_classes[:num_explained_classes]):\n",
    "        if use_abs:\n",
    "            heatmaps[i] = np.abs(heatmaps[i])\n",
    "        heatmaps[i] = heatmaps[i] * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmaps[i])\n",
    "        torch.cuda.empty_cache()\n",
    "    del e,expl\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102fd36-336c-4e34-bcfe-281d59fefa28",
   "metadata": {},
   "source": [
    "### Captum\n",
    "- https://captum.ai/tutorials/TorchVision_Interpret\n",
    "- https://blog.paperspace.com/model-interpretability-and-understanding-for-pytorch-using-captum/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918a99f-707b-4050-9a67-bb3e2f5799f1",
   "metadata": {},
   "source": [
    "### SHAPGrad_Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fb005-674d-45cf-8bff-218a59d04732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jacobgil/pytorch-grad-cam\n",
    "# https://captum.ai/api/gradient_shap.html\n",
    "from captum.attr import GradientShap\n",
    "def get_gradshap_captum_heatmaps(n_samples=50):\n",
    "    if params.pretrained_model_type == 'vit_LRP':\n",
    "        gradient_shap = rand_img_dist = clsid = None\n",
    "        return np.ones((1,224,224))\n",
    "    torch.cuda.empty_cache()\n",
    "    rand_img_dist = torch.cat([image_to_explain_tensor.unsqueeze(0) * 0, image_to_explain_tensor.unsqueeze(0) * 1])\n",
    "    gradient_shap = GradientShap(model)\n",
    "    \n",
    "    heatmaps = []\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        clsid = torch.tensor(clsid)\n",
    "        \n",
    "        heatmap = gradient_shap.attribute(image_to_explain_tensor.unsqueeze(0),\n",
    "                                          n_samples=n_samples,\n",
    "                                          stdevs=0.0001,\n",
    "                                          baselines=rand_img_dist,\n",
    "                                          target=clsid)\n",
    "        heatmap = np.sum(heatmap.squeeze().cpu().detach().numpy(), axis=0)\n",
    "        heatmap = np.abs(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    torch.cuda.empty_cache()\n",
    "    del gradient_shap,rand_img_dist,clsid\n",
    "    return np.array(heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803a4b7-9924-4b7a-961b-bb9bc017568e",
   "metadata": {},
   "source": [
    "## 4. LRP Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9a6b6-cb36-461b-8406-41424fa7a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRP function\n",
    "from captum.attr import LRP\n",
    "def get_LRP_captum_heatmaps():\n",
    "    lrp = LRP(model)\n",
    "    \n",
    "    heatmaps = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        clsid = torch.tensor(clsid).to(device)\n",
    "        # print(clsid,class_names[clsid])\n",
    "        heatmap = lrp.attribute(image_to_explain_tensor.unsqueeze(0),\n",
    "                                target=clsid)\n",
    "        heatmap = heatmap.squeeze().cpu().detach().numpy()\n",
    "        heatmap = np.mean(heatmap, axis=0)\n",
    "        heatmap = np.abs(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    torch.cuda.empty_cache()\n",
    "    del lrp,clsid\n",
    "    return np.array(heatmaps)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e995d2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "if pretrained_model_type  != 'swin_trans_vit' and pretrained_model_type  != 'vit_LRP':\n",
    "    heatmaps_LRP = get_LRP_captum_heatmaps()\n",
    "    heatmap = heatmaps_LRP[0]\n",
    "    vmax = np.quantile(heatmap, q=0.999)\n",
    "    print('vmax',vmax)\n",
    "    plt.subplot(121); plt.imshow(image_to_explain);\n",
    "    plt.subplot(122); plt.imshow(heatmap, vmin = -vmax, vmax = vmax,cmap=shap_bpt.shapley_values_colormap); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334fb37-3911-4dd8-94f2-a12e471a2a7b",
   "metadata": {},
   "source": [
    "### ViT-LRP\n",
    " - https://github.com/hila-chefer/Transformer-Explainability/blob/main/Transformer_explainability.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c670b1-df03-410d-a134-b4bc75d7e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params.pretrained_model_type)\n",
    "if params.pretrained_model_type  == 'vit_LRP': #or pretrained_model_type  == 'swin_trans_vit':\n",
    "    sys.path.append(f'{path_repos}/Transformer-Explainability')\n",
    "    \n",
    "    from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP \n",
    "    #deit_base_patch16_224, vit_large_patch16_224\n",
    "    from baselines.ViT.ViT_explanation_generator import LRP\n",
    "    \n",
    "    use_thresholding =  False#@param {type:\"boolean\"}\n",
    "    \n",
    "    sys.path.append(path_utils)\n",
    "    import helper_functions_VT2 as hfvt\n",
    "    import importlib\n",
    "    \n",
    "    importlib.reload(hfvt)\n",
    "    CLS2IDX = hfvt.imgnet_cl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5fd06-7f53-48a6-962b-3e0b28f3cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71ae50-5dea-47c9-819b-3ba86efa9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmaps_LRP_ViT():\n",
    "    torch.cuda.empty_cache()\n",
    "    if params.pretrained_model_type  == 'swin_trans_vit'  or params.pretrained_model_type  == 'vit':\n",
    "        return np.zeros((1,224,224))\n",
    "        model_lrp = vit_LRP(pretrained=True).to(device)\n",
    "        model_lrp.eval()\n",
    "        attribution_generator = LRP(model)\n",
    "    elif params.pretrained_model_type  == 'vit_LRP':\n",
    "        attribution_generator = LRP(model)\n",
    "    else:\n",
    "        attribution_generator = LRP(model)\n",
    "    \n",
    "    heatmaps_lrp = []\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        clsid = torch.tensor(clsid).to(device)\n",
    "        \n",
    "        transformer_attribution = attribution_generator.generate_LRP(image_to_explain_preproc.unsqueeze(0).cuda(), \n",
    "                                                                     method=\"transformer_attribution\", \n",
    "                                                                     index=clsid).detach()\n",
    "        transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "        transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, \n",
    "                                                                  scale_factor=16, \n",
    "                                                                  mode='bilinear')\n",
    "        transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
    "        transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    \n",
    "        if use_thresholding:\n",
    "          transformer_attribution = transformer_attribution * 255\n",
    "          transformer_attribution = transformer_attribution.astype(np.uint8)\n",
    "          ret, transformer_attribution = cv2.threshold(transformer_attribution, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "          transformer_attribution[transformer_attribution == 255] = 1\n",
    "        heatmaps_lrp.append(transformer_attribution)\n",
    "    torch.cuda.empty_cache()\n",
    "    del clsid,transformer_attribution,attribution_generator\n",
    "    return np.array(heatmaps_lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f354a02-00c4-4cc6-9006-848597c93003",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image_to_explain(files[100], bg_type=params.background_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff4230-d918-43d0-addd-1313d78cab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.pretrained_model_type  == 'swin_trans_vit' or params.pretrained_model_type  == 'vit':\n",
    "    heatmaps_LRP = get_heatmaps_LRP_ViT()\n",
    "    heatmap = heatmaps_LRP[0]\n",
    "    vmax = np.quantile(heatmap, q=0.999)\n",
    "    print('vmax',vmax)\n",
    "    plt.subplot(121); plt.imshow(image_to_explain);\n",
    "    plt.subplot(122); plt.imshow(heatmap, vmin = -vmax, vmax = vmax,cmap=shap_bpt.shapley_values_colormap); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b303115-e433-4d85-92cc-1a42497d9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133115d",
   "metadata": {},
   "source": [
    "## 5. LIME\n",
    " - https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_lime_heatmaps(segments, expl):\n",
    "    global predicted_fS, predicted_f0\n",
    "    class_heatmaps = []\n",
    "    for clsid in expl.top_labels:\n",
    "        heatmap = np.zeros_like(segments, dtype=np.float32)\n",
    "        for segm, importance in expl.local_exp[clsid]:\n",
    "            heatmap[ segments==segm ] += importance \n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        class_heatmaps.append(heatmap)\n",
    "    return np.array(class_heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966225bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_number(image, md):\n",
    "    segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4, max_dist=md, ratio=0.2, random_seed=1234) \n",
    "    segments = segmentation_fn(image)\n",
    "    return len(np.unique(segments))\n",
    "\n",
    "def search_segment_number(image, target_seg_no, init_max_dist=100):\n",
    "    lmd, rmd = 0, init_max_dist\n",
    "    lsn = get_segment_number(image, lmd)\n",
    "    rsn = get_segment_number(image, rmd)\n",
    "    niter = 0\n",
    "    while niter<40 and rsn!=target_seg_no:\n",
    "        niter += 1\n",
    "        mmd = (lmd + rmd) / 2.0\n",
    "        msn = get_segment_number(image, mmd)\n",
    "        if msn <= target_seg_no <= lsn:\n",
    "            rsn, rmd = msn, mmd\n",
    "        else:\n",
    "            lsn, lmd = msn, mmd\n",
    "    return rmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict(img):\n",
    "    global model_type\n",
    "    if params.model_type=='ideal':        \n",
    "        return f_masked_ideal(torch.Tensor(img).permute(0,3,1,2).cpu().numpy() [:,0,:,:])\n",
    "    else:\n",
    "        return f(torch.Tensor(img).permute(0,3,1,2).to(device))\n",
    "    \n",
    "    # return f(torch.Tensor(img).permute(0,3,1,2).to(device))\n",
    "\n",
    "def get_lime_heatmaps(num_segments=100, num_samples=1000, use_stratification=False,verbose=False):\n",
    "    segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4, \n",
    "                                            max_dist=search_segment_number(image_to_explain, num_segments), \n",
    "                                            ratio=0.2, random_seed=1234) \n",
    "                                            \n",
    "    segments = segmentation_fn(image_to_explain)\n",
    "    def segments_getter(img):\n",
    "        return segments\n",
    "    num_segments = len(np.unique(segments))\n",
    "    heatmap_list = []\n",
    "    for bg_c in background_image_preproc_set:\n",
    "        torch.cuda.empty_cache()\n",
    "        lime_explainer = lime_image.LimeImageExplainer(random_state=1234)\n",
    "        lime_expl = lime_explainer.explain_instance(image_to_explain_preproc.permute(1,2,0).detach().numpy(), \n",
    "                                                    lime_predict,\n",
    "                                                    top_labels=num_explained_classes,\n",
    "                                                    # use_stratification=use_stratification,\n",
    "                                                    segmentation_fn=segments_getter,\n",
    "                                                    hide_color=bg_c.permute(1,2,0).detach().numpy(), \n",
    "                                                    num_samples=num_samples,\n",
    "                                                    progress_bar=verbose\n",
    "                                                    )\n",
    "        if isinstance(lime_expl, tuple):\n",
    "            lime_expl = lime_expl[2]\n",
    "        heatmap_list.append(format_lime_heatmaps(segments, lime_expl))\n",
    "        torch.cuda.empty_cache()\n",
    "        del lime_expl,lime_explainer\n",
    "    del segments\n",
    "    return np.mean(heatmap_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7da09",
   "metadata": {},
   "source": [
    "## 6. Integrated Decision Gradients (IDG)\n",
    " - https://github.com/chasewalker26/Integrated-Decision-Gradients/tree/main/util/attribution_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffee175-e5d0-4e02-b179-0624d8ba13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2305.20052.pdf\n",
    "# https://github.com/chasewalker26/Integrated-Decision-Gradients/tree/main\n",
    "sys.path.append(f'{path_repos}/Integrated-Decision-Gradients/util/attribution_methods')\n",
    "\n",
    "from saliencyMethods import IDG\n",
    "def get_idg_heatmaps(use_abs=True):\n",
    "    # global predicted_fS, predicted_f0\n",
    "    steps = 50\n",
    "    batch_si = 25 # default 25\n",
    "    baseline = 0\n",
    "    heatmaps = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        heatmap = idg = IDG(torch.unsqueeze(image_to_explain_tensor, dim=0), model, \n",
    "                            steps, batch_si, baseline, device, predicted_cls)\n",
    "        heatmap = idg.detach().cpu().numpy()\n",
    "        heatmap = np.mean(heatmap, axis=0) # reduce to one attribution per pixel\n",
    "        # normalize\n",
    "        if use_abs:\n",
    "            heatmap = np.abs(heatmap)\n",
    "        # heatmap -= np.min(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    torch.cuda.empty_cache()\n",
    "    del steps,baseline,batch_si\n",
    "    return np.array(heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34eb976",
   "metadata": {},
   "source": [
    "## 7. GradCAM\n",
    " - https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907979fe-cd6c-42b1-bff2-fed537de4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params.pretrained_model_type  == 'swin_trans_vit'\n",
    "\n",
    "# import pytorch_grad_cam\n",
    "\n",
    "# from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, XGradCAM, EigenCAM, LayerCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe09d8",
   "metadata": {},
   "source": [
    "### GradCAM for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "# def get_GradCAM_ViT(verbose=False):\n",
    "#     if verbose:\n",
    "#         print('RUNNING NEW GRADCAM FOR VIT')\n",
    "    \n",
    "#     target_layer = model.encoder.layers[-1].ln_1\n",
    "#     # Initialize GradCAM\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f'USING LAYER {target_layer}')\n",
    "#     cam = GradCAM(model=model, target_layers=[target_layer], reshape_transform=vit_reshape_transform)\n",
    "#     image_to_explain_tensor_4d = torch.unsqueeze(image_to_explain_preproc, dim=0). to(device)\n",
    "\n",
    "#     # Predict\n",
    "#     target_category = torch.argmax(model(image_to_explain_tensor_4d)).item()\n",
    "\n",
    "#     print('target_category', target_category)\n",
    "#     # Generate CAM\n",
    "#     grayscale_cam = cam(input_tensor=image_to_explain_tensor_4d,\n",
    "#                         targets=[ClassifierOutputTarget(target_category)])\n",
    "\n",
    "#     print('grayscale_cam', grayscale_cam.shape)\n",
    "\n",
    "#     grayscale_cam = grayscale_cam[0, :]\n",
    "#     return grayscale_cam\n",
    "\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# GradCAM function\n",
    "def get_gradcam_heatmaps_for_vit(verbose = False):\n",
    "    global model\n",
    "    # print('get_gradcam_heatmaps_for_swin_vit')\n",
    "    global predicted_fS, predicted_f0\n",
    "    # target_layers =  [model.layers[-1].blocks[-1].norm1]\n",
    "    if verbose:\n",
    "        print('RUNNING NEW GRADCAM FOR VIT')\n",
    "    \n",
    "    target_layers = [model.encoder.layers[-1].ln_1]\n",
    "\n",
    "    cam = GradCAM(model=model, \n",
    "                  target_layers=target_layers,\n",
    "                  reshape_transform=vit_reshape_transform)\n",
    "    heatmaps = []\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        heatmap = cam(input_tensor=torch.unsqueeze(image_to_explain_tensor, dim=0), \n",
    "                      targets=[ClassifierOutputTarget(clsid)])[0]\n",
    "        # normalize\n",
    "        # heatmap -= np.mean(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    del cam,clsid\n",
    "    return np.array(heatmaps)\n",
    "\n",
    "\n",
    "# grayscale_cam = get_GradCAM_ViT()\n",
    "# cam_image = show_cam_on_image(image_to_explain / 255.0, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# plt.imshow(cam_image)\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Grad-CAM on ViT-B/16\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44641f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[-1].blocks[-1].norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790083e2-cad9-4586-a0b9-14a101132649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import GuidedGradCam\n",
    "def get_gradcam_vit_heatmaps(num_explained_classes=4):\n",
    "\n",
    "    print('CALLING THIS FUNCTION', get_gradcam_vit_heatmaps, ' for -->', params.pretrained_model_type)\n",
    "    if params.pretrained_model_type  == 'vit_LRP':\n",
    "        return np.zeros((num_explained_classes,224,224))\n",
    "    # elif params.pretrained_model_type  == 'swin_trans_vit':\n",
    "    #     return np.zeros((num_explained_classes,224,224))\n",
    "        # return np.full((1,224,224), 1)\n",
    "    # target_layer = model.norm\n",
    "\n",
    "    # target_layers = [model.blocks[-1].attn]\n",
    "    target_layer = model.encoder.ln   # FOR VIT\n",
    "    # target_layer = [model.norm1]\n",
    "    # target_layer = model.head\n",
    "    # print('target_layer', target_layer)\n",
    "    # target_layer = model.blocks[-1]#.norm2  # Typically, the last layer norm after the attention block\n",
    "    \n",
    "    guided_gc = GuidedGradCam(model, target_layer)\n",
    "\n",
    "    # print(guided_gc)\n",
    "    heatmaps = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        \n",
    "        attribution = guided_gc.attribute(torch.unsqueeze(image_to_explain_tensor, dim=0), 3)\n",
    "        print('attribution', attribution.shape, attribution.dtype)\n",
    "\n",
    "        # print(attribution.shape)\n",
    "        heatmap = np.sum(attribution.squeeze().cpu().detach().numpy(), axis=0)\n",
    "\n",
    "        print('heatmap', heatmap)\n",
    "        heatmap = np.abs(heatmap)\n",
    "        print(clsid, np.sum(heatmap))\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap,attribution\n",
    "    torch.cuda.empty_cache()\n",
    "    del clsid,guided_gc,target_layer\n",
    "    return np.array(heatmaps)\n",
    "get_gradcam_vit_heatmaps(num_explained_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112490a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model.encoder.layers[-1].self_attention]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bbb50-ed11-4a5c-bc3f-1a40f521106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.pretrained_model_type  == 'swin_trans_vit':\n",
    "    heatmap = get_gradcam_vit_heatmaps()[0]\n",
    "    print('heatmap shape:', heatmap.shape)\n",
    "    # print(get_gradcam_vit_heatmaps().shape)\n",
    "    plt.imshow(image_to_explain); plt.show()\n",
    "    print(predicted_cls)\n",
    "    vm = np.quantile(np.abs(heatmap), q=0.99)\n",
    "    plt.imshow(heatmap, vmin = -vm, vmax = vm, cmap= shap_bpt.shapley_values_colormap)\n",
    "\n",
    "if params.pretrained_model_type  == 'vit':\n",
    "    heatmap = get_gradcam_heatmaps_for_vit()[0]\n",
    "    print('heatmap shape:', heatmap.shape)\n",
    "    # print(get_gradcam_vit_heatmaps().shape)\n",
    "    plt.imshow(image_to_explain); plt.show()\n",
    "    print(predicted_cls)\n",
    "    vm = np.quantile(np.abs(heatmap), q=0.99)\n",
    "    plt.imshow(heatmap, vmin = -vm, vmax = vm, cmap= shap_bpt.shapley_values_colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.pretrained_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jacobgil/pytorch-grad-cam\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "if params.pretrained_model_type != 'swin_trans_vit':\n",
    "    if params.pretrained_model_type == 'vgg16':\n",
    "        target_layers = [model.features[-1]]\n",
    "    elif params.pretrained_model_type == 'vit_LRP':\n",
    "        target_layers = [model.blocks[-1].attn]\n",
    "    # if :\n",
    "    #     target_layers = [model.layer4[-1]]\n",
    "        # pass\n",
    "        # target_layers = [model.encoder.layers[-1].self_attention]\n",
    "elif params.pretrained_model_type == 'vit':\n",
    "    target_layers = [model.encoder.layers[-1].self_attention]\n",
    "# GradCAM function\n",
    "def get_gradcam_heatmaps():\n",
    "    global predicted_fS, predicted_f0\n",
    "    \n",
    "    cam = GradCAM(model=model.to(device), \n",
    "                  target_layers=target_layers)\n",
    "    heatmaps = []\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        heatmap = cam(input_tensor=torch.unsqueeze(image_to_explain_tensor, dim=0), \n",
    "                      targets=[ClassifierOutputTarget(clsid)])[0]\n",
    "        # normalize\n",
    "        # heatmap -= np.mean(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    del cam,clsid\n",
    "    return np.array(heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jacobgil/pytorch-grad-cam\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "if params.pretrained_model_type != 'swin_trans_vit':\n",
    "    if params.pretrained_model_type == 'vgg16':\n",
    "        target_layers = [model.features[-1]]\n",
    "    elif params.pretrained_model_type == 'vit_LRP':\n",
    "        target_layers = [model.blocks[-1].attn]\n",
    "\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        # target_layers = [model.blocks[-1].attn]\n",
    "        # target_layers = [model.encoder.layers[-1].self_attention]\n",
    "if  params.pretrained_model_type == 'vit':\n",
    "    target_layers = [model.encoder.layers[-1].self_attention]\n",
    "\n",
    "\n",
    "def swinT_reshape_transform(tensor, height=7, width=7):\n",
    "    result = tensor.reshape(tensor.size(0),\n",
    "                            height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "def vit_reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "# GradCAM function\n",
    "def get_gradcam_heatmaps_for_swin_vit():\n",
    "    global model\n",
    "    # print('get_gradcam_heatmaps_for_swin_vit')\n",
    "    global predicted_fS, predicted_f0\n",
    "    target_layers =  [model.layers[-1].blocks[-1].norm1]\n",
    "    \n",
    "    def reshape_transform(tensor, height=7, width=7):\n",
    "        result = tensor.reshape(tensor.size(0),\n",
    "            height, width, tensor.size(2))\n",
    "\n",
    "        # Bring the channels to the first dimension,\n",
    "        # like in CNNs.\n",
    "        result = result.transpose(2, 3).transpose(1, 2)\n",
    "        return result\n",
    "\n",
    "    cam = GradCAM(model=model, \n",
    "                  target_layers=target_layers,\n",
    "                  reshape_transform=reshape_transform)\n",
    "    heatmaps = []\n",
    "    for clsid in sorted_classes[:num_explained_classes]:\n",
    "        heatmap = cam(input_tensor=torch.unsqueeze(image_to_explain_tensor, dim=0), \n",
    "                      targets=[ClassifierOutputTarget(clsid)])[0]\n",
    "        # normalize\n",
    "        # heatmap -= np.mean(heatmap)\n",
    "        heatmap = heatmap * (predicted_fS[clsid] - predicted_f0[clsid]) / np.sum(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "        del heatmap\n",
    "    del cam,clsid\n",
    "    return np.array(heatmaps)\n",
    "\n",
    "\n",
    "# plt.imshow(get_gradcam_heatmaps_for_swin_vit(model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0106997",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image_to_explain(files[0], bg_type=params.background_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4bfa6-b5b0-4951-92a1-f00eee2224b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.pretrained_model_type  != 'swin_trans_vit' and params.pretrained_model_type  != 'vit_LRP' and params.pretrained_model_type  != 'vit':\n",
    "    heatmap = get_gradcam_heatmaps()[0]\n",
    "    plt.imshow(image_to_explain); plt.show()\n",
    "    plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exp_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9f813-aa45-4c37-8d23-bcb235eb40fd",
   "metadata": {},
   "source": [
    "# Combining All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.pretrained_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7289c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     name,                 color,                  functor\n",
    "verbose = False\n",
    "# if pretrained_model_type  == 'swin_trans_vit':\n",
    "#     verbose = True\n",
    "\n",
    "methods = [\n",
    "    ('BPT-100',         'xkcd:light pink',     partial(get_bpt_heatmaps, num_samples=100,verbose=verbose)),\n",
    "    ('BPT-500',         'xkcd:bright pink',     partial(get_bpt_heatmaps, num_samples=500,verbose=verbose)),\n",
    "    ('BPT-1000',         'xkcd:deep pink',     partial(get_bpt_heatmaps, num_samples=1000,verbose=verbose)),\n",
    "    ]\n",
    "methods_pe = [ # if single background\n",
    "    ('Partition-100',   'xkcd:bluish',     partial(get_pe_heatmaps, num_samples=100)),\n",
    "    ('Partition-500',   'xkcd:cerulean',   partial(get_pe_heatmaps, num_samples=500)),\n",
    "    ('Partition-1000',   'xkcd:soft blue', partial(get_pe_heatmaps, num_samples=1000))\n",
    "    ]\n",
    "methods_aa = [ # if multiple backgrounds\n",
    "    ('AA-100', 'xkcd:bright blue',     partial(get_aa_heatmaps, num_samples=100, verbose=verbose)),\n",
    "    ('AA-500', 'xkcd:bright blue',     partial(get_aa_heatmaps, num_samples=500, verbose=verbose)),\n",
    "    ('AA-1000', 'xkcd:bright blue',    partial(get_aa_heatmaps, num_samples=1000, verbose=verbose)),\n",
    "]\n",
    "methods_aa_huge = [\n",
    "    ('AA-5000', 'xkcd:bright blue',      partial(get_aa_heatmaps, num_samples=5000, verbose=verbose)),\n",
    "    ('AA-10000', 'xkcd:bright blue',     partial(get_aa_heatmaps, num_samples=10000, verbose=verbose)),\n",
    "]\n",
    "methods_lime = [\n",
    "    ('LIME-100',        'xkcd:bright lime',     partial(get_lime_heatmaps, num_segments=100/5, num_samples=100,verbose=verbose)),\n",
    "    ('LIME-500',        'xkcd:kermit green',   partial(get_lime_heatmaps, num_segments=500/5, num_samples=500,verbose=verbose)),\n",
    "    ('LIME-1000',        'xkcd:dark lime green',partial(get_lime_heatmaps, num_segments=1000/5, num_samples=1000,verbose=verbose))\n",
    "    ]\n",
    "\n",
    "methods_cam = [\n",
    "    ('aIDG',         'xkcd:indigo',          partial(get_idg_heatmaps, use_abs=True)),\n",
    "    ('aGradExpl',    'red',                  partial(get_gradexpl_heatmap, use_abs=True))\n",
    "    ]\n",
    "\n",
    "methods_ShapGradE = [\n",
    "    ('ShapGradE',     'xkcd:camel',            partial(get_gradshap_captum_heatmaps, n_samples=20)),    \n",
    "]\n",
    "\n",
    "# methods_limeSAM = [\n",
    "#     ('LIMESAM',        'xkcd:bright lime',     partial(get_limeSAM_heatmaps, num_samples=500, verbose=verbose)),\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n",
    "methods_LRP_ViT = [\n",
    "        ('LRP',        'xkcd:bright lime',     get_heatmaps_LRP_ViT),\n",
    "    \n",
    "    ]\n",
    "methods_LRP = [\n",
    "    ('LRP',        'xkcd:bright lime',     get_LRP_captum_heatmaps),\n",
    "    \n",
    "    ]\n",
    "\n",
    "method_gradcam_vit_heatmaps = [\n",
    "    ('GradCAM',     'xkcd:camel',            get_gradcam_heatmaps_for_swin_vit), #get_gradcam_vit_heatmaps),\n",
    "    ]\n",
    "\n",
    "if params.pretrained_model_type == 'vit_LRP':\n",
    "    methods_gradcam = [\n",
    "    ('GradCAM',     'xkcd:camel',            get_gradcam_vit_heatmaps),\n",
    "    ]\n",
    "elif params.pretrained_model_type == 'vit':\n",
    "    methods_gradcam = [\n",
    "    ('GradCAM',     'xkcd:camel',            get_gradcam_heatmaps_for_vit),\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    methods_gradcam = [\n",
    "    ('GradCAM',     'xkcd:camel',            get_gradcam_heatmaps),\n",
    "    ]\n",
    "    \n",
    "\n",
    "methods += methods_pe if len(background_tensors) == 1 and params.model_type!='ideal' else methods_aa\n",
    "# if params.model_type == 'ideal':\n",
    "#     methods += methods_aa_huge\n",
    "\n",
    "methods += methods_lime\n",
    "# methods += methods_limeSAM\n",
    "######## CAM   #############    \n",
    "# if pretrained_model_type  != 'swin_trans_vit' :\n",
    "\n",
    "# if params.pretrained_model_type  == 'swin_trans_vit' or params.pretrained_model_type == 'vit_LRP' and \\\n",
    "#                          params.pretrained_model_type  != 'vit'\\\n",
    "#       and params.model_type != 'ideal':\n",
    "#     methods += methods_LRP_ViT\n",
    "\n",
    "# else:\n",
    "#     if params.model_type != 'ideal':\n",
    "#         methods += methods_LRP\n",
    "\n",
    "\n",
    "if params.pretrained_model_type  == 'vit' or params.pretrained_model_type  == 'swin_trans_vit':\n",
    "    methods += methods_LRP_ViT\n",
    "# else:\n",
    "    \n",
    "\n",
    "    \n",
    "if params.pretrained_model_type  == 'swin_trans_vit' or params.pretrained_model_type == 'vit_LRP' and params.model_type != 'ideal':\n",
    "    methods += method_gradcam_vit_heatmaps\n",
    "else:\n",
    "    if params.model_type != 'ideal':\n",
    "        methods += methods_gradcam\n",
    "\n",
    "if params.model_type != 'ideal':\n",
    "    methods += methods_cam\n",
    "if params.model_type != 'ideal':\n",
    "    methods += methods_ShapGradE\n",
    "\n",
    "for n,_,functt in methods:\n",
    "    print(n, functt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f408ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exp_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91155eb9-5136-4041-9acc-b59aed340548",
   "metadata": {},
   "source": [
    "# Functions Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_plot_heatmaps(heatmaps,methods_,path=None,title = None,plot_title=False,fontsize=14,plot_colorbar=False,save_fig=True,selected_ext='svg',dpi=200,transparent=True,destroy_fig=True):\n",
    "    fig,axes = plt.subplots(1, len(methods_)+2, figsize=(2*(len(methods_)+2), 2))\n",
    "    axes[0].imshow(image_to_explain)\n",
    "    axes[0].set_xticks([]) ; axes[0].set_yticks([])\n",
    "    axes[1].imshow(ground_truth, cmap='binary')\n",
    "    axes[1].set_xticks([]) ; axes[1].set_yticks([])\n",
    "    if plot_title:\n",
    "        axes[0].set_title('Input', fontsize=fontsize)\n",
    "        axes[1].set_title('GT', fontsize=fontsize)\n",
    "\n",
    "    for ii, (n,c,_) in enumerate(methods_):\n",
    "        ax = axes[ii+2]\n",
    "        vmax = np.quantile(np.abs(heatmaps[n][0]), 0.99)\n",
    "        ax.imshow(heatmaps[n][0], cmap=shap_bpt.shapley_values_colormap, vmin=-vmax, vmax=vmax)\n",
    "        # ax.imshow(heatmaps[n][0], cmap=shap_bpt.colormap_default, vmin=-vmax, vmax=vmax)\n",
    "        \n",
    "        marked_h = mark_boundaries(np.tile((255,255,255,0), (heatmaps[n][0].shape[0],heatmaps[n][0].shape[1],1)), ground_truth, \n",
    "                             mode='thick', color=(0,0,0,1))\n",
    "        ax.imshow(marked_h)\n",
    "        ax.set_xticks([]) ; ax.set_yticks([])\n",
    "        if plot_title:\n",
    "            ax.set_title(n, fontsize=fontsize)\n",
    "    plt.subplots_adjust(wspace=0.05,hspace=0.05)\n",
    "    # plt.tight_layout(pad = 0.1)\n",
    "    \n",
    "    if save_fig:\n",
    "        if title is None:\n",
    "            suffix = f'{image_n}_{params.background_type}_heatmap.{selected_ext}'\n",
    "        else:\n",
    "            suffix = f'{image_n}_{params.background_type}_heatmap_{title}.{selected_ext}'\n",
    "            # print('saving at',path)                                        \n",
    "        plt.savefig(f'{path}//{suffix}',dpi=200,transparent=True,bbox_inches='tight', pad_inches=0.02)\n",
    "    if destroy_fig:\n",
    "        plt.close(fig)\n",
    "    plt.show()\n",
    "    del vmax,marked_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4028edf",
   "metadata": {},
   "source": [
    "### FUNC: fun_plot_IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "props         = dict(boxstyle='round', facecolor='white', alpha=0.6, pad=0.2)\n",
    "def fun_plot_IoU(heatmaps,IoU,methods_,path=None,title = None,plot_title=False,fontsize=14,save_fig=True,selected_ext='svg',dpi=200,transparent=True,destroy_fig=True):\n",
    "    fig,axs = plt.subplots(1,  len(methods_)+2, figsize=(2*(len(methods_)+2), 2))\n",
    "    axs[0].imshow(image_to_explain)\n",
    "    axs[0].set_xticks([]) ; axs[0].set_yticks([])\n",
    "    axs[1].imshow(ground_truth, cmap='binary')\n",
    "    axs[1].set_xticks([]) ; axs[1].set_yticks([])\n",
    "    if plot_title:\n",
    "        axs[0].set_title('input', fontsize=fontsize)\n",
    "        axs[1].set_title('GT', fontsize=fontsize)\n",
    "    \n",
    "    # n = 'BPT-100'\n",
    "    # img, max_IoU = vis_IoU(heatmaps[n][0], IoU[n][2], ground_truth), np.max(IoU[n][1])\n",
    "    # plt.imshow(img)\n",
    "    # return\n",
    "    for ii, (n,_,_) in tqdm(enumerate(methods_), desc='IoU',leave=False):\n",
    "        ax = axs[ii+2]\n",
    "        img, max_IoU = vis_IoU(heatmaps[n][0], IoU[n][2], ground_truth), np.max(IoU[n][1])\n",
    "        # print(img)\n",
    "        ax.imshow(img)\n",
    "        ax.text(11,47, f'IoU:{max_IoU:.3}', fontsize=24, bbox=props,weight='bold')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if plot_title:\n",
    "            ax.set_title(n, fontsize=fontsize)\n",
    "    plt.subplots_adjust(wspace=0.05,hspace=0.05)\n",
    "    # plt.tight_layout(pad = 0.1)\n",
    "    if save_fig:\n",
    "        if title is None:\n",
    "            suffix = f'{image_n}_{params.background_type}_IoU.{selected_ext}'\n",
    "        else:\n",
    "            suffix = f'{image_n}_{params.background_type}_IoU_{title}.{selected_ext}'\n",
    "\n",
    "        plt.savefig(f'{path}//{suffix}',dpi=dpi,transparent=transparent,bbox_inches='tight', pad_inches=0.02)\n",
    "    if destroy_fig:\n",
    "        plt.close(fig)\n",
    "    plt.show()\n",
    "    del img,max_IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda516fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('text',usetex=True)\n",
    "rc('text.latex', preamble='\\\\usepackage{color}')\n",
    "\n",
    "def fun_plot_performance(aucI, aucD, IoU,\n",
    "                         list_variants=['BPT-100', 'AA-100', 'LIME-100'],\n",
    "                         budget_set='100',\n",
    "                         path=None,\n",
    "                         file_name='',\n",
    "                         save_fig=True,\n",
    "                         fontsize=14,\n",
    "                         set_title=False,\n",
    "                         plot_lime=False,\n",
    "                         ttl=None,\n",
    "                         layout='row'  # 'row' or '2rows'\n",
    "                         ):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Order of plots (grouped by column for 2-row layout)\n",
    "    plot_defs = [\n",
    "        {'title': '$\\\\mathit{AUC}^{+}$',         'typee': 'auc',        'from': 'aucI'}, # 0\n",
    "        {'title': '$\\\\mathit{AUC}^{-}$',         'typee': 'auc',        'from': 'aucD'}, # 1\n",
    "        {'title': '$\\\\mathit{AUC}^{+}-clip$',     'typee': 'auc_clip',   'from': 'aucI'}, # 2\n",
    "        {'title': '$\\\\mathit{AUC}^{-}-clip$',     'typee': 'auc_clip',   'from': 'aucD'}, # 3\n",
    "        {'title': '$\\\\mathit{AUC^{+}-Adj}$',     'typee': 'auc_adj',    'from': 'aucI'}, # 4\n",
    "        {'title': '$\\\\mathit{AUC^{-}-Adj}$',     'typee': 'auc_adj',    'from': 'aucD'}, # 5\n",
    "        \n",
    "        {'title': '$\\\\mathit{AU}{\\!-\\!}\\\\mathit{IoU}$', 'typee': 'auc_iou', 'from': 'iou'}, # 6 \n",
    "        #####################################################################\n",
    "\n",
    "        {'title': '$\\\\mathit{AUC}^{+}-r$',       'typee': 'auc_r',      'from': 'aucI'}, # 7\n",
    "        {'title': '$\\\\mathit{AUC}^{-}-r$',       'typee': 'auc_r',      'from': 'aucD'}, # 8\n",
    "        {'title': '$\\\\mathit{AUC}^{+}-clip-r$', 'typee': 'auc_clip_r', 'from': 'aucI'}, # 9\n",
    "        {'title': '$\\\\mathit{AUC}^{-}-clip-r$', 'typee': 'auc_clip_r', 'from': 'aucD'}, # 10\n",
    "        {'title': '$\\\\mathit{AUC^{+}-Adj-r}$',   'typee': 'auc_adj_r',  'from': 'aucI'}, # 11\n",
    "        {'title': '$\\\\mathit{AUC^{-}-Adj-r}$',   'typee': 'auc_adj_r',  'from': 'aucD'}, # 12 \n",
    "        \n",
    "    ]\n",
    "    save_version = list_variants[0].split('-')[-1]\n",
    "    # Reorder plot_defs for 2-row layout: interleaved columns\n",
    "    if layout == '2rows':\n",
    "        top = plot_defs[0::2]\n",
    "        bottom = plot_defs[1::2]\n",
    "        plot_defs = [None]*(len(top)+len(bottom))\n",
    "        plot_defs[::2] = top\n",
    "        plot_defs[1::2] = bottom\n",
    "\n",
    "    total_variant = len(plot_defs)\n",
    "\n",
    "    # Layout config\n",
    "    if layout == 'row':\n",
    "        nrows, ncols = 1, total_variant\n",
    "        fig_width, fig_height = 25, 2.5\n",
    "    elif layout == '2rows':\n",
    "        nrows, ncols = 2, int(np.ceil(total_variant / 2))\n",
    "        fig_width, fig_height = 16, 5\n",
    "    else:\n",
    "        raise ValueError(\"layout must be either 'row' or '2rows'\")\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Alias\n",
    "    aucI_aa = aucI[list_variants[0]]\n",
    "    aucD_aa = aucD[list_variants[0]]\n",
    "    auc_IoU_aa = IoU[list_variants[0]]\n",
    "\n",
    "    aucI_bpt = aucI[list_variants[1]]\n",
    "    aucD_bpt = aucD[list_variants[1]]\n",
    "    auc_IoU_bpt = IoU[list_variants[1]]\n",
    "\n",
    "    aucI_lime = aucI[list_variants[2]]\n",
    "    aucD_lime = aucD[list_variants[2]]\n",
    "    auc_IoU_lime = IoU[list_variants[2]]\n",
    "\n",
    "    def get_params_auc(auc_, typee):\n",
    "        return {\n",
    "            'auc':        (auc_['xs'], auc_['ys'],     auc_['ms'], auc_['auc']),\n",
    "            'auc_r':      (auc_['xs'], auc_['ysr'],    auc_['ms'], auc_['auc_r']),\n",
    "            'auc_adj':    (auc_['xs'], auc_['y_adj'],  auc_['ms'], auc_['auc_adj']),\n",
    "            'auc_adj_r':  (auc_['xs'], auc_['y_adjr'], auc_['ms'], auc_['auc_adjr']),\n",
    "            'auc_clip':   (auc_['xs'], auc_['y_clip'], auc_['ms'], auc_['auc_clip']),\n",
    "            'auc_clip_r': (auc_['xs'], auc_['y_clipr'],auc_['ms'], auc_['auc_clipr']),\n",
    "        }.get(typee, (None, None, None, None))\n",
    "\n",
    "    def get_params_iou(iou_, typee):\n",
    "        if typee == 'auc_iou':\n",
    "\n",
    "            # xmb, ymb = iou_[3], np.max(iou_[1])\n",
    "\n",
    "\n",
    "            Xa, Ya, Ma, La = iou_[0], iou_[1], None, iou_[4]\n",
    "            xma, yma = iou_[3], np.max(iou_[1])\n",
    "            return Xa, Ya, Ma, La, xma, yma\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    for i, config in enumerate(plot_defs):\n",
    "        ax = axes[i]\n",
    "        title = config['title']\n",
    "        typee = config['typee']\n",
    "        source = config['from']\n",
    "\n",
    "        # Select correct dict\n",
    "        if source == 'aucI':\n",
    "            Xa, Ya, Ma, La = get_params_auc(aucI_aa, typee)\n",
    "            Xb, Yb, Mb, Lb = get_params_auc(aucI_bpt, typee)\n",
    "            Xl, Yl, Ml, Ll = get_params_auc(aucI_lime, typee)\n",
    "        elif source == 'aucD':\n",
    "            Xa, Ya, Ma, La = get_params_auc(aucD_aa, typee)\n",
    "            Xb, Yb, Mb, Lb = get_params_auc(aucD_bpt, typee)\n",
    "            Xl, Yl, Ml, Ll = get_params_auc(aucD_lime, typee)\n",
    "        elif source == 'iou':\n",
    "            Xa, Ya, Ma, La, xma, yma = get_params_iou(auc_IoU_aa,  typee)\n",
    "            Xb, Yb, Mb, Lb, xmb, ymb = get_params_iou(auc_IoU_bpt, typee)\n",
    "            Xl, Yl, Ml, Ll, xml, yml = get_params_iou(auc_IoU_lime,   typee)\n",
    "\n",
    "        # Boldness logic\n",
    "        Sa, Sb = ('\\\\textbf', '') if La < Lb else ('', '\\\\textbf')\n",
    "        if i in [0,2,4,6,8]: Sa, Sb = Sb, Sa\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(Xa, Ya, c='#2465a6', label=f'{Sa}{{AA}} {La:.4f}')\n",
    "        ax.fill_between(Xa, Ya, color='#2465a6', alpha=0.15, hatch='///')\n",
    "        ##########\n",
    "        ax.plot(Xb, Yb, c='#9d2f4d', label=f'{Sb}{{BPT}} {Lb:.4f}', alpha=0.80)\n",
    "        ax.fill_between(Xb, Yb, color='#9d2f4d', alpha=0.15, hatch='\\\\\\\\\\\\')\n",
    "        ##########\n",
    "        if plot_lime:\n",
    "            ax.plot(Xl, Yl, c=\"#01B0A7\", label=f'{Sb}{{LM}} {Ll:.4f}', alpha=0.80)\n",
    "            ax.fill_between(Xl, Yl, color='#01B0A7', alpha=0.15, hatch='\\\\\\\\\\\\')\n",
    "\n",
    "\n",
    "        if typee == 'auc_iou':\n",
    "            ax.scatter(xma, yma, s=40, color='blue')\n",
    "            ax.scatter(xmb, ymb, s=40, color='red')\n",
    "\n",
    "        if i < 4:\n",
    "            ax.axhline(0.0, ls='--', c='grey', zorder=0)  # placeholder for f_S - f_0\n",
    "\n",
    "        ax.axhline(0, c='lightgrey', zorder=0)\n",
    "        ax.legend(borderpad=0.2, labelspacing=0.1, loc='upper right' if i>=1 else 'lower right')\n",
    "        ax.set_title(title, fontsize=fontsize)\n",
    "    axes[-1].imshow(image_to_explain); axes[-1].set_xticks([]); axes[-1].set_yticks([])\n",
    "    axes[-1].set_title(ttl.split(\",\")[0])\n",
    "    if set_title:\n",
    "        plt.suptitle(f'{ttl}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig and path is not None:\n",
    "        plt.savefig(f'{path}/{file_name}_{save_version}_paired_auc_metrics.png', dpi=150, bbox_inches='tight', pad_inches=0.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61782526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fun_plot_performance_old(aucI,aucD,IoU,budget_set='100',path=None,save_fig=True,fontsize=14):\n",
    "#     fig,axes = plt.subplots(1,5, figsize=(10,2), sharex=True, sharey=True) #3,3  figsize=(8,2.2)\n",
    "#     if len(background_tensors)==1:\n",
    "#         if params.model_type=='ideal':    \n",
    "#             aucI_aa =  aucI['AA-100']\n",
    "#             aucD_aa =  aucD['AA-100']\n",
    "#             auc_IoU_aa =  IoU['AA-100']\n",
    "#             ttl = 'PE'\n",
    "#         else:\n",
    "#             aucI_aa =  aucI['Partition-100']\n",
    "#             aucD_aa =  aucD['Partition-100']\n",
    "#             auc_IoU_aa =  IoU['Partition-100']\n",
    "#     else:\n",
    "#         aucI_aa =  aucI['AA-100']\n",
    "#         aucD_aa =  aucD['AA-100']\n",
    "#         auc_IoU_aa =  IoU['AA-100']\n",
    "#         ttl = 'AA'\n",
    "#     aucI_bpt =  aucI['BPT-100']\n",
    "#     aucD_bpt =  aucD['BPT-100']\n",
    "#     auc_IoU_bpt =  IoU['BPT-100']\n",
    "    \n",
    "#     for i in range(5):\n",
    "#         ax = axes.flat[i]\n",
    "#         if i==0: # insertion/regression\n",
    "#             # Xa,Ya,Ma,La      = aucI_aa['xs'],aucI_aa['ys'],aucI_aa['ms'],aucI_aa['auc_reg']\n",
    "#             title='$\\\\mathit{AUC}^{+}$'\n",
    "#             Xa,Ya,Ma,La      = aucI_aa ['xs'],aucI_aa ['ys'],aucI_aa ['ms'],aucI_aa ['auc']\n",
    "#             Xb,Yb,Mb,Lb      = aucI_bpt['xs'],aucI_bpt['ys'],aucI_bpt['ms'],aucI_bpt['auc']\n",
    "#             # Xb,Yb,Mb,Lb = aucI_bpt[0], aucI_bpt[1], aucI_bpt[2], aucI_bpt[3]\n",
    "#         elif i==1: # deletion/regression\n",
    "#             title='$\\\\mathit{AUC}^{-}$'\n",
    "#             Xa,Ya,Ma,La      = aucD_aa ['xs'],aucD_aa ['ys'],aucD_aa ['ms'],aucD_aa ['auc']\n",
    "#             Xb,Yb,Mb,Lb      = aucD_bpt['xs'],aucD_bpt['ys'],aucD_bpt['ms'],aucD_bpt['auc']\n",
    "            \n",
    "#         elif i==2: # insertion/error\n",
    "#             title='$\\\\mathit{MSE}^{+}$'\n",
    "#             # Xa,Ya,Ma,La      = aucI_aa ['xs'],aucI_aa ['ys'],aucI_aa ['ms'],aucI_aa ['auc']\n",
    "#             # Xb,Yb,Mb,Lb      = aucI_bpt['xs'],aucI_bpt['ys'],aucI_bpt['ms'],aucI_bpt['auc']\n",
    "\n",
    "#             Xa,Ya,Ma,La = aucI_aa ['xs'], (aucI_aa ['ys']- aucI_aa  ['ms'])**2, aucI_aa  ['ms'], aucI_aa  ['auc_mse']\n",
    "#             Xb,Yb,Mb,Lb = aucI_bpt ['xs'],(aucI_bpt ['ys']-aucI_bpt ['ms'])**2, aucI_bpt ['ms'], aucI_bpt ['auc_mse']\n",
    "            \n",
    "\n",
    "#         elif i==3: # deletion/error\n",
    "#             title='$\\\\mathit{MSE}^{-}$'\n",
    "#             Xa,Ya,Ma,La = aucI_aa ['xs'], (aucI_aa  ['ys']-aucI_aa  ['ms'])**2, aucI_aa  ['ms'], aucI_aa  ['auc_mse']\n",
    "#             Xb,Yb,Mb,Lb = aucI_bpt ['xs'],(aucI_bpt ['ys']-aucI_bpt ['ms'])**2, aucI_bpt ['ms'], aucI_bpt ['auc_mse']\n",
    "            \n",
    "#         elif i==4: # IoU\n",
    "#             title='$\\\\mathit{AU}{\\!-\\!}\\\\mathit{IoU}$'\n",
    "#             Xa,Ya,Ma,La = auc_IoU_aa[0], auc_IoU_aa[1], None, auc_IoU_aa[4]\n",
    "#             Xb,Yb,Mb,Lb = auc_IoU_bpt[0], auc_IoU_bpt[1], None, auc_IoU_bpt[4]\n",
    "#             xma, yma = auc_IoU_aa[3], np.max(auc_IoU_aa[1])\n",
    "#             xmb, ymb = auc_IoU_bpt[3], np.max(auc_IoU_bpt[1])\n",
    "            \n",
    "#     #     ymax = max(np.max(Ya), np.max(Yb))\n",
    "        \n",
    "#         Sa, Sb = ('\\\\textbf', '') if La<Lb else ('', '\\\\textbf')\n",
    "#         if i in [0,4]:   Sa,Sb=Sb,Sa\n",
    "#         ax.plot(Xa, Ya, c='#2465a6', label=f'{Sa}{{AA}} {La:.4}')\n",
    "#         ax.fill_between(Xa, Ya, color='#2465a6', alpha=0.15, hatch='///')\n",
    "#         # ax.scatter(Xa, Ya, c='black', s=5)\n",
    "#         ax.plot(Xb, Yb, c='#9d2f4d', label=f'{Sb}{{BPT}} {Lb:.4}', alpha=0.80)\n",
    "#         ax.fill_between(Xb, Yb, color='#9d2f4d', alpha=0.15, hatch='\\\\\\\\\\\\')\n",
    "        \n",
    "#         if i==4:\n",
    "#             ax.scatter(xma, yma, s=40, color='blue')\n",
    "#             ax.scatter(xmb, ymb, s=40, color='red')\n",
    "\n",
    "#         if i < 2:\n",
    "#             ax.axhline(f_S, ls='--', c='grey', zorder=0)\n",
    "            \n",
    "#         ax.axhline(0, c='lightgrey', zorder=0)\n",
    "#         ax.legend(borderpad=0.2, labelspacing=0.1, loc='upper right' if i>=1 else 'lower right')#, bbox_to_anchor=(1,0))\n",
    "#         ax.set_title(title, fontsize=fontsize)\n",
    "    \n",
    "#     # axes[0].set_xlabel('\\% pixels inserted', fontsize=14)\n",
    "#     # axes[1].set_xlabel('\\% pixels deleted', fontsize=14)\n",
    "#     # axes[2].set_xlabel('\\% pixels inserted', fontsize=14)\n",
    "#     # axes[3].set_xlabel('\\% pixels deleted', fontsize=14)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     if save_fig:\n",
    "#         if path is not None:\n",
    "#             plt.savefig(f'{path}/{image_n}_five_metrics_{params.background_type}_2.png', dpi=150, bbox_inches='tight', pad_inches=0.02)\n",
    "#         else:\n",
    "#             print('path is None')\n",
    "#     # plt.savefig(f'{path}/five_metrics_{background_type}_2.svg', dpi=150, bbox_inches='tight', pad_inches=0.02)\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e5a53",
   "metadata": {},
   "source": [
    "### Plot and save Partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_plot_heatmaps_partial(methods_, heatmaps, filter_methods=None, path=None, title=None, plot_title=False, \n",
    "                              fontsize=14, plot_colorbar=False, save_fig=True, selected_ext='svg', dpi=200, \n",
    "                              transparent=True, destroy_fig=True):\n",
    "    \n",
    "    # Save Input Image\n",
    "    if save_fig:\n",
    "        fig, ax = plt.subplots(figsize=(2, 2))\n",
    "        ax.imshow(image_to_explain)\n",
    "        ax.set_xticks([]) \n",
    "        ax.set_yticks([])\n",
    "        if plot_title:\n",
    "            ax.set_title(\"Input\", fontsize=fontsize)\n",
    "\n",
    "        suffix = f'{image_n}_{params.background_type}_Input.{selected_ext}'\n",
    "        plt.savefig(f'{path}/{suffix}', dpi=dpi, transparent=transparent, bbox_inches='tight', pad_inches=0.02)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Save Ground Truth\n",
    "    if save_fig:\n",
    "        fig, ax = plt.subplots(figsize=(2, 2))\n",
    "        ax.imshow(ground_truth, cmap='binary')\n",
    "        ax.set_xticks([]) \n",
    "        ax.set_yticks([])\n",
    "        if plot_title:\n",
    "            ax.set_title(\"Ground Truth\", fontsize=fontsize)\n",
    "\n",
    "        suffix = f'{image_n}_{params.background_type}_GT.{selected_ext}'\n",
    "        plt.savefig(f'{path}/{suffix}', dpi=dpi, transparent=transparent, bbox_inches='tight', pad_inches=0.02)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Save Heatmaps for Each Method\n",
    "    for n, c, _ in methods_:\n",
    "        if filter_methods is None or n in filter_methods:\n",
    "            fig, ax = plt.subplots(figsize=(2, 2))  # Create a new figure for each method\n",
    "            \n",
    "            vmax = np.quantile(np.abs(heatmaps[n][0]), 0.99)\n",
    "            ax.imshow(heatmaps[n][0], cmap=shap_bpt.shapley_values_colormap, vmin=-vmax, vmax=vmax)\n",
    "\n",
    "            # Overlay ground truth boundaries\n",
    "            marked_h = mark_boundaries(\n",
    "                np.tile((255,255,255,0), (heatmaps[n][0].shape[0], heatmaps[n][0].shape[1], 1)), \n",
    "                ground_truth, mode='thick', color=(0, 0, 0, 1)\n",
    "            )\n",
    "            ax.imshow(marked_h)\n",
    "            \n",
    "            ax.set_xticks([]) \n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            if plot_title:\n",
    "                ax.set_title(n, fontsize=fontsize)\n",
    "\n",
    "            plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "            \n",
    "            # Save figure with method name\n",
    "            if save_fig:\n",
    "                if title is None:\n",
    "                    suffix = f'{image_n}_Part_{params.background_type}_{n}_heatmap.{selected_ext}'\n",
    "                else:\n",
    "                    suffix = f'{image_n}_Part_{params.background_type}_{n}_heatmap_{title}.{selected_ext}'\n",
    "                \n",
    "                plt.savefig(f'{path}/{suffix}', dpi=dpi, transparent=transparent, bbox_inches='tight', pad_inches=0.02)\n",
    "\n",
    "            if destroy_fig:\n",
    "                plt.close(fig)\n",
    "\n",
    "            del vmax, marked_h  # Clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_plot_heatmaps_filtered(methods_, heatmaps, filter_methods=None, path=None, title=None, plot_title=False, \n",
    "                               fontsize=14, plot_colorbar=False, save_fig=True, selected_ext='svg', dpi=200, \n",
    "                               transparent=True, destroy_fig=True):\n",
    "    \n",
    "    # Filter methods\n",
    "    selected_methods = [n for n, _, _ in methods_ if filter_methods is None or n in filter_methods]\n",
    "    num_methods = len(selected_methods)\n",
    "\n",
    "    # If no methods are selected, exit\n",
    "    if num_methods == 0:\n",
    "        print(\"No methods selected for filtering. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Total subplots: input + ground truth + filtered methods\n",
    "    num_subplots = num_methods + 2\n",
    "    fig, axs = plt.subplots(1, num_subplots, figsize=(2.5 * num_subplots, 2.5))\n",
    "\n",
    "    # Plot Input Image\n",
    "    axs[0].imshow(image_to_explain)\n",
    "    axs[0].set_xticks([]) \n",
    "    axs[0].set_yticks([])\n",
    "    if plot_title:\n",
    "        axs[0].set_title(\"Input\", fontsize=fontsize)\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    axs[1].imshow(ground_truth, cmap='binary')\n",
    "    axs[1].set_xticks([]) \n",
    "    axs[1].set_yticks([])\n",
    "    if plot_title:\n",
    "        axs[1].set_title(\"Ground Truth\", fontsize=fontsize)\n",
    "\n",
    "    # Plot Selected Heatmaps\n",
    "    for i, n in enumerate(selected_methods):\n",
    "        ax = axs[i + 2]\n",
    "        \n",
    "        vmax = np.quantile(np.abs(heatmaps[n][0]), 0.99)\n",
    "        ax.imshow(heatmaps[n][0], cmap=shap_bpt.shapley_values_colormap, vmin=-vmax, vmax=vmax)\n",
    "\n",
    "        # Overlay ground truth boundaries\n",
    "        marked_h = mark_boundaries(\n",
    "            np.tile((255,255,255,0), (heatmaps[n][0].shape[0], heatmaps[n][0].shape[1], 1)), \n",
    "            ground_truth, mode='thick', color=(0, 0, 0, 1)\n",
    "        )\n",
    "        ax.imshow(marked_h)\n",
    "        \n",
    "        ax.set_xticks([]) \n",
    "        ax.set_yticks([])\n",
    "\n",
    "        if plot_title:\n",
    "            ax.set_title(n, fontsize=fontsize)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    # Save the complete figure\n",
    "    if save_fig:\n",
    "        if title is None:\n",
    "            suffix = f'{image_n}_Filtered_{params.background_type}_Heatmaps.{selected_ext}'\n",
    "        else:\n",
    "            suffix = f'{image_n}_Filtered_{params.background_type}_Heatmaps_{title}.{selected_ext}'\n",
    "        \n",
    "        plt.savefig(f'{path}/{suffix}', dpi=dpi, transparent=transparent, bbox_inches='tight', pad_inches=0.02)\n",
    "\n",
    "    if destroy_fig:\n",
    "        plt.close(fig)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_plot_IoU_filtered(methods_, IoU, heatmaps, filter_methods=None, path=None, title=None, plot_title=False, \n",
    "                          fontsize=14, save_fig=True, selected_ext='svg', dpi=200, transparent=True, destroy_fig=True):\n",
    "    # print('RUNNING fun_plot_IoU_filtered')\n",
    "    # Filter methods based on user selection\n",
    "    selected_methods = [(n, c, v) for n, c, v in methods_ if filter_methods is None or n in filter_methods]\n",
    "    num_methods = len(selected_methods)\n",
    "\n",
    "    # If no methods are selected, exit\n",
    "    if num_methods == 0:\n",
    "        print(\"No methods selected for filtering. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Total subplots: input + ground truth + filtered methods\n",
    "    num_subplots = num_methods + 2\n",
    "    fig, axs = plt.subplots(1, num_subplots, figsize=(2.5 * num_subplots, 2.5))\n",
    "\n",
    "    # Plot Input Image\n",
    "    axs[0].imshow(image_to_explain)\n",
    "    axs[0].set_xticks([]) \n",
    "    axs[0].set_yticks([])\n",
    "    if plot_title:\n",
    "        axs[0].set_title(\"Input\", fontsize=fontsize)\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    axs[1].imshow(ground_truth, cmap='binary')\n",
    "    axs[1].set_xticks([]) \n",
    "    axs[1].set_yticks([])\n",
    "    if plot_title:\n",
    "        axs[1].set_title(\"Ground Truth\", fontsize=fontsize)\n",
    "\n",
    "    # Plot IoU Visualizations for Selected Methods\n",
    "    for i, (n, _, _) in enumerate(selected_methods):\n",
    "        ax = axs[i + 2]\n",
    "        \n",
    "        img, max_IoU = vis_IoU(heatmaps[n][0], IoU[n][2], ground_truth), np.max(IoU[n][1])\n",
    "        ax.imshow(img)\n",
    "        ax.text(10, 30, f'IoU: {max_IoU:.3}', fontsize=fontsize, bbox=props, weight='bold')\n",
    "\n",
    "        ax.set_xticks([]) \n",
    "        ax.set_yticks([])\n",
    "\n",
    "        if plot_title:\n",
    "            ax.set_title(n, fontsize=fontsize)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    # Save the complete figure\n",
    "    if save_fig:\n",
    "        if title is None:\n",
    "            suffix = f'{image_n}_{params.background_type}_Filtered_IoU.{selected_ext}'\n",
    "        else:\n",
    "            suffix = f'{image_n}_{params.background_type}_Filtered_IoU_{title}.{selected_ext}'\n",
    "        \n",
    "        plt.savefig(f'{path}/{suffix}', dpi=dpi, transparent=transparent, bbox_inches='tight', pad_inches=0.02)\n",
    "\n",
    "    if destroy_fig:\n",
    "        plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    print(f'{\"model_type\":<25} {params.model_type}')\n",
    "    print(f'{\"pretrained_model_type\":<25} {params.pretrained_model_type}')\n",
    "    print(f'{\"background counts\":<25} {len(background_tensors)} , {params.background_type}')\n",
    "    print(f'{\"Num of Methods\":<25} {len(methods)}')\n",
    "    print(f'{\"Device\":<25} {device}')\n",
    "\n",
    "def check_groundtruth(ground_truth):\n",
    "    imagenetS_classes = set(np.unique(ground_truth))    \n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    return imagenetS_classes\n",
    "def fix_groundtruth(imagenetS_classes):\n",
    "    assert len(imagenetS_classes)==1\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    return class_mask_id,imagenet_class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_fixed(ground_truth):\n",
    "    global imagenetS_classes\n",
    "    imagenetS_classes = set(np.unique(ground_truth))\n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    return imagenetS_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a80f3f",
   "metadata": {},
   "source": [
    "## Organize Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_full_selected = []\n",
    "verbose_print = False\n",
    "\n",
    "print(f'{\"Image\":<30} | {class_names[predicted_cls]:<20} | {f_S:10.4} {f_0:10.4}  |  {f_G:10.4}  |  {f_B:10.4}')\n",
    "\n",
    "for image_name in tqdm(files, desc='CHECKING FILES AND GROUNDTRUTH'):\n",
    "\n",
    "    data_time,data = {} , {}\n",
    "    image_n = image_name.split('.')[0].split('//')[-1]\n",
    "\n",
    "    load_image_to_explain(f'{image_name}', bg_type=params.background_type)\n",
    "    imagenetS_classes = set(np.unique(ground_truth))\n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    if len(imagenetS_classes)>1:\n",
    "        if verbose_print:\n",
    "            print(f'classes more than 1 for image: {image_n} - count: {len(imagenetS_classes)}')\n",
    "        continue\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    if  imagenet_class_id!=predicted_cls:\n",
    "        if verbose_print:\n",
    "            print(f'{image_n},{predicted_cls}!={imagenet_class_id} unmatched')\n",
    "        continue\n",
    "    if verbose_print:\n",
    "        print(f'{image_n:<30} | {class_names[predicted_cls]:<20} | {f_S:10.4} {f_0:10.4}  |  {f_G:10.4}  |  {f_B:10.4}')\n",
    "    \n",
    "    img_path = f'{path_img_val}/{image_n}.JPEG'\n",
    "    if os.path.exists(image_name) and os.path.exists(img_path):\n",
    "\n",
    "        file_full_selected.append([image_name, img_path])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath_img, filepath_gt in file_full_selected:\n",
    "    print(os.path.exists(filepath_img), os.path.exists(filepath_gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de9596",
   "metadata": {},
   "source": [
    "# CREATE DATASET FOR SELECTED EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "verbose_print = False\n",
    "\n",
    "selected_dataset  = os.path.join(path_ds_main, 'ImageNet_selected')\n",
    "\n",
    "selected_dataset_imgs = os.path.join(selected_dataset, 'images')\n",
    "selected_dataset_gts = os.path.join(selected_dataset, 'groundtruths')\n",
    "\n",
    "os.makedirs(selected_dataset_imgs, exist_ok=True)\n",
    "os.makedirs(selected_dataset_gts, exist_ok=True)\n",
    "\n",
    "for filepath_gt, filepath_img in tqdm(file_full_selected, desc=' Copying Files'):\n",
    "   \n",
    "    if os.path.exists(filepath_img):\n",
    "        destination_path = os.path.join(selected_dataset_imgs, filepath_img.split('/')[-1])\n",
    "        if verbose_print:\n",
    "            print('FROM ', filepath_img, ' To ', destination_path)\n",
    "        shutil.copy(filepath_img, destination_path)\n",
    "\n",
    "    if os.path.exists(filepath_gt):\n",
    "        destination_path = os.path.join(selected_dataset_gts, filepath_gt.split('//')[-1])\n",
    "        if verbose_print:\n",
    "            print('FROM ', filepath_gt, ' To ', destination_path)\n",
    "        shutil.copy(filepath_gt, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e1881",
   "metadata": {},
   "source": [
    "# Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7925aa-6689-4f93-ac9d-bcc456b357d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_full_function(file_selected=8733):\n",
    "        #  image_name class_label\n",
    "    #  47683      sulphur_butterfly \n",
    "    #  17635      ladybug \n",
    "    #  5428       street_sign \n",
    "    #  8733       park_bench \n",
    "    global image_n\n",
    "    file_selected = 8733\n",
    "    global image_to_explain, ground_truth, predicted_cls, class_names, f_S, f_0, sel_image_name_id\n",
    "    fs_st = f\"ILSVRC2012_val_{file_selected:08}\"\n",
    "    \n",
    "    for image_name_id, image_name in enumerate(tqdm(files)):\n",
    "        image_n = image_name.split('.')[0].split('//')[-1]\n",
    "        if fs_st==image_n:\n",
    "            sel_image_name_id = image_name_id\n",
    "            # print(image_name_id)\n",
    "            load_image_to_explain(files[image_name_id], bg_type =params.background_type)\n",
    "            print(image_name_id, class_names[predicted_cls], f_S, predicted_cls, f_0)\n",
    "            print(f'background counts\\t:\\t{len(background_tensors)} , {params.background_type}')\n",
    "\n",
    "load_image_full_function(file_selected=8733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f598c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,_,_ in methods:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f924e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_cam_only  = ['GradCAM','aIDG', 'aGradExpl', 'ShapGradE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60669133-d2c3-48e1-9cb0-40d4e5b1364a",
   "metadata": {},
   "source": [
    "## COMPUTE EXPLANATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f09104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "verbose = True\n",
    "plot_partial = False\n",
    "st_full = time.time()\n",
    "evaluate_explanation = True\n",
    "# load_image_to_explain(files[image_name_id], bg_type =background_type)\n",
    "\n",
    "#  image_name class_label\n",
    "#  47683      sulphur_butterfly \n",
    "#  17635      ladybug \n",
    "#  5428       street_sign \n",
    "#  8733       park_bench \n",
    "\n",
    "load_image_full_function(file_selected=8733)\n",
    "\n",
    "# get_headers()\n",
    "get_exp_header()\n",
    "print(f'{\"f_S\":<25} {f_S}')\n",
    "\n",
    "st_full = time.time()\n",
    "# imagenetS_classes = set(np.unique(ground_truth))\n",
    "# imagenetS_classes.discard(0)\n",
    "# imagenetS_classes.discard(1000)\n",
    "\n",
    "imagenetS_classes = get_gt_fixed(ground_truth)\n",
    "\n",
    "if len(imagenetS_classes)==1:\n",
    "    assert len(imagenetS_classes)==1\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    # print(imagenet_class_id, predicted_cls, imagenet_class_id==predicted_cls)\n",
    "    if  imagenet_class_id==predicted_cls:\n",
    "    \n",
    "        f_S = float(predicted_fS[predicted_cls])\n",
    "        class_tag = classid_to_tag[predicted_cls]\n",
    "        ground_truth = ground_truth == class_mask_id\n",
    "        heatmaps, aucD, aucI,IoU = {}, {}, {},{}\n",
    "        for n,_,funct in tqdm(methods):\n",
    "            # if params.pretrained_model_type=='swin_trans_vit'and n=='GradCAM':\n",
    "            #     continue\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            # if n == 'BPT-1000' or n == 'BPT-500' or n == 'AA-1000' or n == 'AA-500' or n == 'Partition-1000' or n == 'Partition-500' or n == 'LIME-500' or n == 'LIME-1000':\n",
    "                # continue\n",
    "            # if n not in methods_cam_only:\n",
    "            #     continue  \n",
    "\n",
    "            # if n!='aIDG' or n!='aIDG':\n",
    "                # continue\n",
    "            print('method : ',n)\n",
    "            # print_gpu_memory()\n",
    "            \n",
    "            heatmaps[n] = funct()\n",
    "\n",
    "            # print('method in loop: ',heatmaps[n].shape, heatmaps[n]) \n",
    "            if plot_partial:\n",
    "                plt.figure(figsize=(2,2))\n",
    "                vmax = np.quantile(np.abs(heatmaps[n][0]), 0.99)\n",
    "                plt.imshow(heatmaps[n][0], cmap=shap_bpt.shapley_values_colormap, vmin=-vmax, vmax=vmax)\n",
    "                plt.xticks([]); plt.yticks([]) \n",
    "                plt.show()\n",
    "                del vmax\n",
    "            if evaluate_explanation:\n",
    "                st = time.time()\n",
    "                aucD[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='del', batch_size=batch_size)\n",
    "                aucI[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='ins', batch_size=batch_size)\n",
    "                IoU[n]  = calc_IoU_curve(ground_truth.flatten(), heatmaps[n][0].flatten())\n",
    "        del n,funct\n",
    "        print('-'*120)\n",
    "time_single_full = time.time()-st_full\n",
    "print(f'for bg_type {len(background_tensors)} and no_methods : {len(methods)} on device {device} -  time taken :\\t {time_single_full} and could take {time_single_full*545}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d02c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_variants=['BPT-100', 'Partition-100', 'LIME-100'] if params.pretrained_model_type!='swin_trans_vit' else ['BPT-100', 'Partition-100', 'LIME-100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa607b-8cb1-42ed-b75f-5e73a169ea9c",
   "metadata": {},
   "source": [
    "## Plotting Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colorbar = False\n",
    "plot_title    = True\n",
    "save_fig      = True\n",
    "fontsize      = 14\n",
    "print(results_path_single)\n",
    "# get_headers()\n",
    "get_exp_header()\n",
    "print('-'*120)\n",
    "print(f'{\"method\":<20}{\"Explained Prob\":<15}')\n",
    "for ii, (n,c,_) in enumerate(methods):\n",
    "    if n!='aIDG':\n",
    "        continue\n",
    "    print(f'{n:<20}{np.sum(heatmaps[n][0])}')\n",
    "fun_plot_heatmaps(heatmaps,methods,path=results_path_single,destroy_fig=False, selected_ext='png', \n",
    "                  save_fig=save_fig,plot_colorbar=plot_colorbar,plot_title=plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231eee5f-7e78-4e6d-8335-62049049a306",
   "metadata": {},
   "source": [
    "## Visualize IoU and GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9584a-05ca-44c7-a6fe-df7101a387dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_title = True\n",
    "save_fig   = True\n",
    "get_exp_header()\n",
    "fun_plot_IoU(heatmaps,IoU,methods,path=results_path_single,destroy_fig=False, selected_ext='png',plot_title=plot_title, save_fig=save_fig,fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.pretrained_model_type, params.model_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cecae2e-5e84-4860-953b-0b5e60185a5b",
   "metadata": {},
   "source": [
    "## PLOT AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exp_header()\n",
    "\n",
    "AA_name = 'Partition' if params.model_type!='ideal' else 'AA'\n",
    "list_variants=['BPT-100', f'{AA_name}-100', 'LIME-100']\n",
    "\n",
    "\n",
    "plt_ttl = f'{image_n}, explained_class: {predicted_cls} '\\\n",
    "                            f'f_S:\\ {f_S:0.6}, f_0: {f_0:0.6}, delta: {abs(f_S-f_0):0.6}\\n'\\\n",
    "                            f'f_G: {f_G:0.6}, f_B: {f_B:0.6}'\n",
    "\n",
    "fun_plot_performance(aucI,aucD,IoU,list_variants=list_variants,path=results_path, file_name=image_n,layout='2rows',\n",
    "                                     set_title=True,ttl=plt_ttl)\n",
    "# fun_plot_performance(aucI,aucD,IoU,path=results_path_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exp_header()\n",
    "AA_name = 'Partition' if params.model_type!='ideal' else 'AA'\n",
    "\n",
    "list_variants=['BPT-500', f'{AA_name}-500', 'LIME-500']\n",
    "\n",
    "plt_ttl = f'{image_n}, explained_class: {predicted_cls} '\\\n",
    "                            f'f_S:\\ {f_S:0.6}, f_0: {f_0:0.6}, delta: {abs(f_S-f_0):0.6}\\n'\\\n",
    "                            f'f_G: {f_G:0.6}, f_B: {f_B:0.6}'\n",
    "\n",
    "fun_plot_performance(aucI,aucD,IoU,list_variants=list_variants,\n",
    "                     path=results_path, file_name=image_n,layout='2rows',\n",
    "                     set_title=True,ttl=plt_ttl)\n",
    "# fun_plot_performance(aucI,aucD,IoU,path=results_path_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exp_header()\n",
    "AA_name = 'Partition' if params.model_type!='ideal' else 'AA'\n",
    "\n",
    "\n",
    "list_variants=['BPT-1000', f'{AA_name}-1000', 'LIME-1000']\n",
    "\n",
    "plt_ttl = f'{image_n}, explained_class: {predicted_cls} '\\\n",
    "                            f'f_S:\\ {f_S:0.6}, f_0: {f_0:0.6}, delta: {abs(f_S-f_0):0.6}\\n'\\\n",
    "                            f'f_G: {f_G:0.6}, f_B: {f_B:0.6}'\n",
    "\n",
    "fun_plot_performance(aucI,aucD,IoU,list_variants=list_variants,\n",
    "                     path=results_path, file_name=image_n,layout='2rows',\n",
    "                     set_title=True,ttl=plt_ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898bfca",
   "metadata": {},
   "source": [
    "# FUNCT: create log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b45dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_file(plots_path__,start_time, verbose=False):\n",
    "    # Define log messages\n",
    "    log_messages = \"\"\n",
    "    log_save_data_msg = f\"saving data :\\t{plots_path__}\\n\"\n",
    "    log_code_started_msg = f\"code started:\\t{start_time}\\n\"\n",
    "    log_table_header_msg = f\"{'Image_no': <25} {'predicted_class': <22} {'predicted_prob': <20} {'class_id': <15} {'background_class': <15}\\n\"\n",
    "    log_header_main = f\"| {'FILENAME':22} | {'MODEL_TYPE':15} | {'PRETRAINED_MODEL':22} | {'BACKGROUND_TYPE':22} | {'Total':22} |\\n\"\n",
    "    log_header_main_data = f\"| {params.file_name:22} | {params.model_type:15} | {params.pretrained_model_type:22} | {params.background_type:22} | {str(len(files)):22} |\\n\"\n",
    "    log_separator_dash = \"-\" * 120 + \"\\n\"\n",
    "    log_separator_bold = \"=\" * 120 + \"\\n\"\n",
    "    \n",
    "    # Append log messages\n",
    "    log_messages += log_separator_bold\n",
    "    log_messages += log_header_main\n",
    "    log_messages += log_header_main_data\n",
    "    log_messages += log_separator_dash\n",
    "    log_messages += log_save_data_msg\n",
    "    log_messages += log_code_started_msg\n",
    "    log_messages += log_separator_dash\n",
    "    log_messages += log_table_header_msg\n",
    "\n",
    "    # Print separator lines and log messages\n",
    "    if verbose:\n",
    "        print(log_separator_bold.strip())\n",
    "        print(log_header_main.strip())\n",
    "        print(log_header_main_data.strip())\n",
    "    \n",
    "        print(log_separator_bold.strip())\n",
    "        print(log_save_data_msg.strip())\n",
    "        print(log_code_started_msg.strip())\n",
    "        \n",
    "        print(log_table_header_msg.strip())\n",
    "        print(log_separator_dash.strip())\n",
    "    return log_messages\n",
    "\n",
    "# log_messages = create_log_file(results_path_selected,start_time)\n",
    "# print(log_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a8ec9",
   "metadata": {},
   "source": [
    "# RUN TEST ON SELECTED IMAGES for Figures\n",
    "Run below two cells to produce data for **Figure 4**, **Figure 6**,  **Figure 8**,**Figure 10**, and **Figure 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a94cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f0bfc-3052-4fa2-a2a0-bbbec1ce4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if params.file_name=='exp_E1':\n",
    "    figure_no = 4     # exp \n",
    "elif params.file_name=='exp_E2':\n",
    "    figure_no = 8     # exp e2\n",
    "elif params.file_name=='exp_E1_ViT':\n",
    "    figure_no = 8     # exp e2\n",
    "elif params.file_name=='exp_E3':\n",
    "    figure_no = 8     # exp e3\n",
    "elif params.file_name=='exp_E4':\n",
    "    figure_no = 10    # exp e4\n",
    "elif params.file_name=='exp_E5':\n",
    "    figure_no = 8 #12      # exp e5\n",
    "else:\n",
    "    figure_no = 8\n",
    "######################################################################\n",
    "\n",
    "# figure_no = 4     # exp \n",
    "# figure_no = 6     # exp \n",
    "# figure_no = 8     # exp e2\n",
    "# figure_no = 10    # exp e4\n",
    "# figure_no = 12      # exp e5\n",
    "# figure_no = 13 # Multi Objects\n",
    "######################################################################\n",
    "select_set = 'selected' # selected , full \n",
    "######################################################################\n",
    "if select_set=='selected':\n",
    "    log_file_selected = f'{results_path_selected}/log_file.txt'\n",
    "    # plotsIoU_path = plots_path = results_path_selected\n",
    "    # results_path   = results_path_selected\n",
    "    selected_ext  = 'svg'\n",
    "    if figure_no is None:\n",
    "        file_selected = [17505]\n",
    "    elif figure_no==4:\n",
    "        file_selected = [618, 17635, 5428, 8733]\n",
    "\n",
    "    elif figure_no==6:\n",
    "        file_selected = [17505,3843,4203,25140,7684,8292,11346,20075]    # Figure 6 Experiment E1 Extended\n",
    "        #file_selected = [3843]\n",
    "    elif figure_no==8:\n",
    "        file_selected = [17505,3843,4203,25140,7684,8292,11346,20075,618,17635,5428,8733,1739,3223]                 # Figure 8, Experiment E2\n",
    "    elif figure_no==13:\n",
    "        file_selected = [28713,37846]\n",
    "    else:\n",
    "        file_selected = [47683,17635,5428,8733]                          \n",
    "    file_selected_ls = []    \n",
    "    for fs in file_selected:\n",
    "        fs_st = f\"ILSVRC2012_val_{fs:08}\"\n",
    "        file_selected_ls.append(fs_st)\n",
    "\n",
    "print('file_name:\\t',      params.file_name)\n",
    "print('select_set:\\t',      select_set)\n",
    "print('figure_no:\\t',        figure_no)\n",
    "print('file_selected:\\t',file_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f1e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n,_,_ in methods]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbeb11",
   "metadata": {},
   "source": [
    "## RUN FOR PROBABILITY OF SINGLE EXMAPLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0002a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_selected_ls = []\n",
    "# file_selected = [25140]\n",
    "# # file_selected = [47683]\n",
    "\n",
    "# for fs in file_selected:\n",
    "#     fs_st = f\"ILSVRC2012_val_{fs:08}\"\n",
    "#     file_selected_ls.append(fs_st)\n",
    "\n",
    "\n",
    "\n",
    "# files_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_selected = []\n",
    "\n",
    "\n",
    "for image_name in files:\n",
    "    image_n = image_name.split('.')[0].split('//')[-1]\n",
    "    if select_set=='selected' and image_n not in file_selected_ls:\n",
    "        continue\n",
    "    files_selected.append(image_name)\n",
    "files_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name in tqdm(files_selected):\n",
    "\n",
    "    data_time,data = {} , {}\n",
    "    image_n = image_name.split('.')[0].split('//')[-1]\n",
    "    if select_set=='selected' and image_n not in file_selected_ls:\n",
    "        continue\n",
    "    #print(image_n, file_selected_ls)\n",
    "    st_load = time.time()\n",
    "    load_image_to_explain(f'{image_name}', bg_type=params.background_type)\n",
    "    single_img_name = image_name\n",
    "    imagenetS_classes = set(np.unique(ground_truth))\n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    if len(imagenetS_classes)>1:\n",
    "        print('classes more than 1')\n",
    "        continue\n",
    "    assert len(imagenetS_classes)==1\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    # table_header_msg = f\"{'Image no': <{25}} {'predicted class': <{22}} {'predicted prob': <{20}} {'predicted class-id': <{15}} {'background class': <{15}}\\n\"\n",
    "    table_msg = f\"{image_n.split('_')[2]: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20.6}} | {np_softmax(predicted_fS)[predicted_cls]: <{20.6}} | {predicted_cls: <{15}} {f_0: <{15}}\\n\"\n",
    "    log_messages += table_msg\n",
    "    print(table_msg.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34db301-2108-4fda-9e9d-074450eea652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_to_csv =  []\n",
    "\n",
    "plot_heatmaps            = True\n",
    "plot_IoU                 = True\n",
    "save_fig                 = True\n",
    "destroy_fig              = False\n",
    "save_vectors_heatmaps    = True\n",
    "save_vectors_evaluation  = False\n",
    "plot_title               = False\n",
    "st_full                  = time.time()\n",
    "\n",
    "# disallowed_vectors = ['LRP','GradCAM','aIDG','aGradExpl','ShapGradE']\n",
    "# if model_type=='real':\n",
    "\n",
    "# filter_methods = ['AA-500','Partition-500','BPT-500','LIME-500']\n",
    "\n",
    "filter_methods = [n for n,_,_ in methods]\n",
    "\n",
    "# disallowed_vectors = []\n",
    "methods_ls,methods_ls_id = [],[]\n",
    "for i_id,(i,_,_) in enumerate(methods):\n",
    "    i = i.replace('Partition','PE')\n",
    "    i = i.replace('GradCAM','GC')\n",
    "    i = i.replace('-','_')\n",
    "    methods_ls.append(i)\n",
    "    methods_ls_id.append(i_id+1)\n",
    "files_success =[]\n",
    "start_time = datetime.now()\n",
    "\n",
    "log_messages = create_log_file(results_path_selected,start_time)\n",
    "print(log_messages)\n",
    "\n",
    "for image_name in tqdm(files_selected):\n",
    "\n",
    "    data_time,data = {} , {}\n",
    "    image_n = image_name.split('.')[0].split('//')[-1]\n",
    "    if select_set=='selected' and image_n not in file_selected_ls:\n",
    "        continue\n",
    "    #print(image_n, file_selected_ls)\n",
    "    st_load = time.time()\n",
    "    load_image_to_explain(f'{image_name}', bg_type=params.background_type)\n",
    "    single_img_name = image_name\n",
    "    imagenetS_classes = set(np.unique(ground_truth))\n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    if len(imagenetS_classes)>1:\n",
    "        print('classes more than 1')\n",
    "        continue\n",
    "    assert len(imagenetS_classes)==1\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    # table_header_msg = f\"{'Image no': <{25}} {'predicted class': <{22}} {'predicted prob': <{20}} {'predicted class-id': <{15}} {'background class': <{15}}\\n\"\n",
    "    table_msg = f\"{image_n.split('_')[2]: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20.6}} {predicted_cls: <{15}} {f_0: <{15}}\\n\"\n",
    "    log_messages += table_msg\n",
    "    print(table_msg.strip())\n",
    "    if  imagenet_class_id!=predicted_cls:\n",
    "        print(image_n, 'other class')\n",
    "        continue\n",
    "\n",
    "    class_tag = classid_to_tag[predicted_cls]\n",
    "    ground_truth = ground_truth == class_mask_id\n",
    "    time_load = time.time()-st_load\n",
    "    ############     HeatMaps    ############\n",
    "    heatmaps = {}\n",
    "    files_success.append(image_n)\n",
    "    time_exp = {}\n",
    "    for n,_,funct in tqdm(methods, desc='Explanation',leave=False):\n",
    "        if n not in filter_methods:\n",
    "            continue\n",
    "        \n",
    "        heatmap_filename = f'{vector_path}//heatmaps_{image_n}_{n}.pkl'\n",
    "        \n",
    "        if save_vectors_heatmaps:\n",
    "            if os.path.exists(heatmap_filename):\n",
    "                # print(f'loaded : {image_n}_{n}.pkl')\n",
    "                st = time.time()\n",
    "                if n in filter_methods:\n",
    "                    with open(heatmap_filename, 'rb') as rf:\n",
    "                        heatmaps[n] = pickle.load(rf)\n",
    "                time_exp[n] = time.time()-st\n",
    "            else:\n",
    "                st = time.time()\n",
    "                heatmaps[n] = funct()\n",
    "                time_exp[n] = time.time()-st\n",
    "                if n in filter_methods:\n",
    "                    # print(n, 'is allowed')\n",
    "                    with open(heatmap_filename, 'wb') as pf:\n",
    "                        pickle.dump(heatmaps[n], pf)\n",
    "                else:\n",
    "                    pass\n",
    "                    # print(n, 'loaded from saved explanation')\n",
    "        else:\n",
    "            st = time.time()\n",
    "            heatmaps[n] = funct()\n",
    "            time_exp[n] = time.time()-st\n",
    "            \n",
    "    if plot_heatmaps:\n",
    "        # fun_plot_heatmaps(heatmaps,methods,path=results_path_selected,destroy_fig=destroy_fig,save_fig=save_fig,plot_title=plot_title, dpi=100)\n",
    "        # fun_plot_heatmaps_partial(methods,heatmaps,filter_methods=filter_methods,path=results_path_selected,destroy_fig=True,save_fig=save_fig,plot_title=plot_title, dpi=100)\n",
    "        fun_plot_heatmaps_filtered(methods,heatmaps,filter_methods=filter_methods,path=results_path_selected,\n",
    "                                   destroy_fig=False,save_fig=save_fig,plot_title=plot_title, dpi=100)\n",
    "        \n",
    "    ############     AUCs    ############\n",
    "    aucD, aucI,IoU = {}, {},{}\n",
    "    overlaps={}\n",
    "    for n,_,_ in tqdm(methods, desc='score',leave=False):\n",
    "        if n not in filter_methods:\n",
    "            continue\n",
    "        aucD_filename    = f'{vector_path}//aucD_{image_n}_{n}.pkl'\n",
    "        aucI_filename    = f'{vector_path}//aucI_{image_n}_{n}.pkl'\n",
    "        aucIoU_filename    = f'{vector_path}//aucIoU_{image_n}_{n}.pkl'\n",
    "        #############     aucD    ############################\n",
    "        st = time.time()\n",
    "        # aucD[n] = saliency_to_auc(heatmaps[n][0], method='del')\n",
    "        aucD[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='del', batch_size=batch_size)\n",
    "               \n",
    "        time_aucD = time.time()-st\n",
    "        ############    aucI     ############################\n",
    "        st = time.time()\n",
    "        # aucI[n] = saliency_to_auc(heatmaps[n][0], method='ins')\n",
    "        aucI[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='ins', batch_size=batch_size)\n",
    "        time_aucI = time.time()-st\n",
    "        st = time.time()\n",
    "        ###########   IOU  ############################\n",
    "        IoU[n] = calc_IoU_curve(ground_truth.flatten(), heatmaps[n][0].flatten())\n",
    "\n",
    "        time_auc_IoU = time.time()-st      \n",
    "        ###############\n",
    "        if save_vectors_evaluation:\n",
    "            with open(aucD_filename, 'wb') as pf:\n",
    "                pickle.dump(aucD, pf)\n",
    "            with open(aucI_filename, 'wb') as pf:\n",
    "                pickle.dump(aucI, pf)\n",
    "            with open(aucIoU_filename, 'wb') as pf:\n",
    "                pickle.dump(IoU, pf)\n",
    "        \n",
    "        data = {'image'         : image_n,\n",
    "                'bg_type'       : params.background_type,\n",
    "                'pred_cls'      : predicted_cls,\n",
    "                'pred_lbl'      : class_names[predicted_cls],\n",
    "                'f_S'           : f_S,\n",
    "                'f_0'           : f_0,\n",
    "                'f_T'           : np.sum(heatmaps[n][0]),\n",
    "                'method'        : n,\n",
    "                'threshold'     : IoU[n][2],\n",
    "                'best_point'    : IoU[n][3],\n",
    "                'max_IoU'       : np.max(IoU[n][1]),\n",
    "                'au_IoU'        : IoU[n][4],\n",
    "                ##############################\n",
    "                'aucI_pred'     : aucI[n]['auc'],  # aucI[n][-3],\n",
    "                'aucD_pred'     : aucD[n]['auc'],  # aucD[n][-3],\n",
    "                ##############################\n",
    "                'aucI_r'     : aucI[n]['auc_r'],  # aucI[n][-3],\n",
    "                'aucD_r'     : aucD[n]['auc_r'],  # aucD[n][-3],\n",
    "                ###\n",
    "                'aucI_adj'     : aucI[n]['auc_adj'],  # aucI[n][-3],\n",
    "                'aucD_adj'     : aucD[n]['auc_adj'],  # aucD[n][-3],\n",
    "                ###\n",
    "                'aucI_adj_r'     : aucI[n]['auc_adjr'],  # aucI[n][-3],\n",
    "                'aucD_adj_r'     : aucD[n]['auc_adjr'],  # aucD[n][-3],\n",
    "                ###\n",
    "                ###############################\n",
    "                # 'aucI_mse'      : aucI[n]['auc_mse'],  # aucI[n][-1],\n",
    "                # 'aucD_mse'      : aucD[n]['auc_mse'],  # aucD[n][-1],\n",
    "                'time_load'     : time_load,\n",
    "                'time_exp'      : time_exp[n],\n",
    "                'time_aucI'     : time_aucI,\n",
    "                'time_aucD'     : time_aucD,\n",
    "                'time_auc_IoU'  : time_auc_IoU,\n",
    "                'time_total'    : time_load+time_exp[n]+time_aucI+time_aucD+time_auc_IoU\n",
    "                }\n",
    "        data_to_csv.append(data)\n",
    "    if plot_IoU:\n",
    "        # fun_plot_IoU(heatmaps,IoU,methods,path=results_path_selected,destroy_fig=destroy_fig,save_fig=save_fig,plot_title=plot_title, dpi=100)\n",
    "        fun_plot_IoU_filtered(methods,IoU, heatmaps,filter_methods=filter_methods,path=results_path_selected,\n",
    "                              destroy_fig=False,save_fig=save_fig,plot_title=plot_title, dpi=100, fontsize = 20)\n",
    "    print('-'*120)\n",
    "    df_selected = pd.DataFrame(data_to_csv)\n",
    "\n",
    "time_full = time.time()-st_full\n",
    "# print(f'Time for {select_set} test is :, {time_full}')\n",
    "\n",
    "end_time = datetime.now()\n",
    "# print('Duration: {}'.format(end_time - start_time))\n",
    "\n",
    "chime.success()\n",
    "\n",
    "time_full_msg = f\"Time for {select_set} test is : {time_full}\\n\"\n",
    "duration_msg = f\"Duration: {format(end_time - start_time)}\\n\"\n",
    "print(duration_msg)\n",
    "print(time_full_msg)\n",
    "\n",
    "log_messages += str(\"=\"*120 + '\\n')\n",
    "log_messages+= str(f'code completed {datetime.now()}\\n')\n",
    "log_messages += duration_msg\n",
    "log_messages += time_full_msg\n",
    "\n",
    "with open(log_file_selected, \"w\") as log_file:\n",
    "    log_file.write(log_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57093794-1a79-4de6-9419-64affb022f92",
   "metadata": {},
   "source": [
    "# Run test on ALL Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c6379-3416-4d0d-a54e-e8d90f53a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_full_test = False\n",
    "run_full_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5862e4-6c06-4d8b-b119-6f51d7774e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_full_test:\n",
    "    log_file_full = f'{results_path}/log_file_full.txt'\n",
    "    file_selected_ls = []\n",
    "    ######################################################################0\n",
    "    select_set = 'full' # selected , partial, full \n",
    "    ######################################################################\n",
    "    if select_set=='full':\n",
    "        selected_ext  = 'png'\n",
    "    \n",
    "    print('selected_ext:\\t',selected_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6715a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = f'{results_path_fixed}/csv_{params.file_name}_{DS_name_img}_{suffix}_{model_softmax}.csv'\n",
    "print(file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78943d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_full_selected = []\n",
    "verbose_print = False\n",
    "\n",
    "for image_name in tqdm(files):\n",
    "\n",
    "    data_time,data = {} , {}\n",
    "    image_n = image_name.split('.')[0].split('//')[-1]\n",
    "    if select_set=='selected' and image_n not in file_selected_ls:\n",
    "        continue\n",
    "    st_load = time.time()\n",
    "    load_image_to_explain(f'{image_name}', bg_type=params.background_type)\n",
    "    imagenetS_classes = set(np.unique(ground_truth))\n",
    "    imagenetS_classes.discard(0)\n",
    "    imagenetS_classes.discard(1000)\n",
    "    if len(imagenetS_classes)>1:\n",
    "        if verbose_print:\n",
    "            print(f'classes more than 1 for image: {image_n} - count: {len(imagenetS_classes)}')\n",
    "        continue\n",
    "    class_mask_id = list(imagenetS_classes)[0]\n",
    "    imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "    table_msg = f'{image_n: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20}} {predicted_cls: <{15}} {f_0: <{15}}\\n'\n",
    "    log_messages += table_msg\n",
    "    if verbose_print:\n",
    "        print(table_msg.strip())\n",
    "        # print(f'{image_n: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20}} {predicted_cls: <{15}} {f_0: <{15}}')\n",
    "    if  imagenet_class_id!=predicted_cls:\n",
    "        if verbose_print:\n",
    "            print(f'{image_n},{predicted_cls}!={imagenet_class_id} unmatched')\n",
    "        continue\n",
    "    \n",
    "    print(f'{image_n} {f_S:0.4} {f_0:10.4} {f_G:10.4} {f_B:10.4}')\n",
    "    file_full_selected.append([image_name, image_n])\n",
    "    print(file_full_selected)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no of success files are ', len(file_full_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c0200-5c6e-4b19-abb6-b29580a028e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_full_test:\n",
    "    datatime_to_csv         = []\n",
    "    data_to_csv             = []\n",
    "    plot_heatmaps           = True\n",
    "    plot_IoU                = True\n",
    "    save_fig                = True\n",
    "    destroy_fig             = True\n",
    "    dpi                     = 50\n",
    "    save_vectors_heatmaps   = False\n",
    "    save_vectors_evaluation = False\n",
    "    plot_title              = False\n",
    "    st_full                 = time.time()\n",
    "    verbose_print           = False\n",
    "\n",
    "    # disallowed_vectors = ['Partition-100','Partition-500','Partition-1000','aGradExpl','GradCAM','LRP']\n",
    "    disallowed_vectors = []\n",
    "    methods_ls,methods_ls_id = [],[]\n",
    "    for i_id,(i,_,_) in enumerate(methods):\n",
    "        i = i.replace('Partition','PE')\n",
    "        i = i.replace('GradCAM','GC')\n",
    "        i = i.replace('-','_')\n",
    "        methods_ls.append(i)\n",
    "        methods_ls_id.append(i_id+1)\n",
    "    files_success =[]\n",
    "    start_time = datetime.now()\n",
    "    # create log file\n",
    "    log_messages = create_log_file(plotsIoU_path,start_time)\n",
    "    if verbose_print:    \n",
    "        print(log_messages)\n",
    "    \n",
    "    for image_name in tqdm(files):\n",
    "    \n",
    "        data_time,data = {} , {}\n",
    "        image_n = image_name.split('.')[0].split('//')[-1]\n",
    "        if select_set=='selected' and image_n not in file_selected_ls:\n",
    "            continue\n",
    "        st_load = time.time()\n",
    "        load_image_to_explain(f'{image_name}', bg_type=params.background_type)\n",
    "        single_img_name = image_name\n",
    "        imagenetS_classes = set(np.unique(ground_truth))\n",
    "        imagenetS_classes.discard(0)\n",
    "        imagenetS_classes.discard(1000)\n",
    "        if len(imagenetS_classes)>1:\n",
    "            if verbose_print:\n",
    "                print('classes more than 1')\n",
    "            continue\n",
    "        assert len(imagenetS_classes)==1\n",
    "        class_mask_id = list(imagenetS_classes)[0]\n",
    "        imagenet_class_id = imagenetS_to_imagenet[class_mask_id]\n",
    "        table_msg = f'{image_n: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20}} {predicted_cls: <{15}} {f_0: <{15}}\\n'\n",
    "        log_messages += table_msg\n",
    "        if verbose_print:\n",
    "            print(table_msg.strip())\n",
    "            # print(f'{image_n: <{25}} {class_names[predicted_cls]: <{22}} {f_S: <{20}} {predicted_cls: <{15}} {f_0: <{15}}')\n",
    "        if  imagenet_class_id!=predicted_cls:\n",
    "            if verbose_print:\n",
    "                print(f'{image_n},{predicted_cls}!={imagenet_class_id} unmatched')\n",
    "            continue\n",
    "        \n",
    "        class_tag = classid_to_tag[predicted_cls]\n",
    "        ground_truth = ground_truth == class_mask_id\n",
    "        ##----------------------------    EVALUATION  -------------------------------------------------##\n",
    "        f_G, f_B = get_bg_values(f_masked,ground_truth.astype(bool),predicted_cls,class_names)\n",
    "        ##-------------------------------------------------------------------------------------------------\n",
    "        time_load = time.time()-st_load\n",
    "        \n",
    "        ############     HeatMaps    ############\n",
    "        heatmaps = {}\n",
    "        files_success.append(image_n)\n",
    "        time_exp = {}\n",
    "        for n,_,funct in tqdm(methods, desc='Explanation',leave=False):\n",
    "            heatmap_filename = f'{vector_path}//heatmaps_{image_n}_{n}.pkl'\n",
    "            # st = time.time()\n",
    "            # heatmaps[n] = funct()\n",
    "            # time_exp[n] = time.time()-st\n",
    "            \n",
    "            if save_vectors_heatmaps:\n",
    "                if os.path.exists(heatmap_filename):\n",
    "                    if verbose_print:\n",
    "                        print(f'loaded : {image_n}_{n}.pkl')\n",
    "                    st = time.time()\n",
    "                    if n not in disallowed_vectors:\n",
    "                        with open(heatmap_filename, 'rb') as rf:\n",
    "                            heatmaps[n] = pickle.load(rf)\n",
    "                    time_exp[n] = time.time()-st\n",
    "                else:\n",
    "                    st = time.time()\n",
    "                    heatmaps[n] = funct()\n",
    "                    time_exp[n] = time.time()-st\n",
    "                    # np.save(heatmap_filename,heatmaps[n], allow_pickle=True)\n",
    "                    if n not in disallowed_vectors:\n",
    "                        # print(n, 'is allowed')\n",
    "                        with open(heatmap_filename, 'wb') as pf:\n",
    "                            pickle.dump(heatmaps[n], pf)\n",
    "                    else:\n",
    "                        pass\n",
    "                        # print(n, 'not allowed to save')\n",
    "            else:\n",
    "                st = time.time()\n",
    "                heatmaps[n] = funct()\n",
    "                time_exp[n] = time.time()-st\n",
    "                \n",
    "        if plot_heatmaps:\n",
    "            fun_plot_heatmaps(heatmaps,methods,path=plots_path,destroy_fig=destroy_fig,selected_ext=selected_ext,\n",
    "                              save_fig=save_fig,plot_title=plot_title, dpi=dpi)\n",
    "        ##---------------------------------------------------------------------------------------------##\n",
    "        ##----------------------------    EVALUATION  -------------------------------------------------##\n",
    "        aucD, aucI,IoU = {}, {},{}\n",
    "        overlaps={}\n",
    "        for n,_,_ in tqdm(methods, desc='Evaluation',leave=False):\n",
    "            aucD_filename    = f'{vector_path}//aucD_{image_n}_{n}.pkl'\n",
    "            aucI_filename    = f'{vector_path}//aucI_{image_n}_{n}.pkl'\n",
    "            aucIoU_filename  = f'{vector_path}//aucIoU_{image_n}_{n}.pkl'\n",
    "            ##---------------------------------------------------------------------------------------------##\n",
    "            ##----------------------------    aucD        -------------------------------------------------##\n",
    "            st = time.time()\n",
    "            aucD[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='del', batch_size=batch_size)\n",
    "            time_aucD = time.time()-st\n",
    "            ##----------------------------    aucI        -------------------------------------------------##\n",
    "            st = time.time()\n",
    "            aucI[n] = saliency_to_auc(f_masked,heatmaps[n][0],f_S, f_0, predicted_cls, method='ins', batch_size=batch_size)\n",
    "            time_aucI = time.time()-st\n",
    "            ##----------------------------    IOU        -------------------------------------------------##\n",
    "            st = time.time()\n",
    "            IoU[n] = calc_IoU_curve(ground_truth.flatten(), heatmaps[n][0].flatten())\n",
    "            time_auc_IoU = time.time()-st\n",
    "            ##---------------------------------------------------------------------------------------------##\n",
    "            if save_vectors_evaluation:\n",
    "                with open(aucD_filename, 'wb') as pf:\n",
    "                    pickle.dump(aucD, pf)\n",
    "                with open(aucI_filename, 'wb') as pf:\n",
    "                    pickle.dump(aucI, pf)\n",
    "                with open(aucIoU_filename, 'wb') as pf:\n",
    "                    pickle.dump(IoU, pf)\n",
    "            ##----------------------------------------------------------------------------------------##\n",
    "            data = {'image'         : image_n,\n",
    "                    'bg_type'       : params.background_type,\n",
    "                    'pred_cls'      : predicted_cls,\n",
    "                    'pred_lbl'      : class_names[predicted_cls],\n",
    "                    ##----------------------------------------------------------------------------------------##\n",
    "                    'f_S'           : f_S,\n",
    "                    'f_0'           : f_0,\n",
    "                    'delta_f'       : f_S-f_0,\n",
    "                    'f_G'           : f_B,\n",
    "                    'f_B'           : f_G,\n",
    "                    'f_T'           : np.sum(heatmaps[n][0]),\n",
    "                    'f_N'           : len(np.unique(heatmaps[n][0])),           ## UNIQUE PATCHES IN EXPLANATION\n",
    "                    'method'        : n,\n",
    "                    ##----------------------------------------------------------------------------------------##\n",
    "                    'aucI_pred'     : aucI[n]['auc'],  \n",
    "                    'aucD_pred'     : aucD[n]['auc'],  \n",
    "                    ##------------------------------------\n",
    "                    'aucI_r'        : aucI[n]['auc_r'],  \n",
    "                    'aucD_r'        : aucD[n]['auc_r'],  \n",
    "                    ##------------------------------------\n",
    "                    'aucI_adj'      : aucI[n]['auc_adj'],  \n",
    "                    'aucD_adj'      : aucD[n]['auc_adj'],  \n",
    "                    ##------------------------------------\n",
    "                    'aucI_adj_r'    : aucI[n]['auc_adjr'],  \n",
    "                    'aucD_adj_r'    : aucD[n]['auc_adjr'],  \n",
    "                    ##------------------------------------\n",
    "                    'aucI_clip'     : aucI[n]['auc_clip'], \n",
    "                    'aucI_clipr'    : aucI[n]['auc_clipr'],\n",
    "                    ##------------------------------------\n",
    "                    'aucD_clip'     : aucD[n]['auc_clip'], \n",
    "                    'aucD_clipr'    : aucD[n]['auc_clipr'],\n",
    "                    ##------------------------------------\n",
    "                    'threshold'     : IoU[n][2],\n",
    "                    'best_point'    : IoU[n][3],\n",
    "                    'max_IoU'       : np.max(IoU[n][1]),\n",
    "                    'au_IoU'        : IoU[n][4],\n",
    "                    ##----------------------------------------------------------------------------------------##\n",
    "                    'time_load'     : time_load,\n",
    "                    'time_exp'      : time_exp[n],\n",
    "                    'time_aucI'     : time_aucI,\n",
    "                    'time_aucD'     : time_aucD,\n",
    "                    'time_auc_IoU'  : time_auc_IoU,\n",
    "                    'time_total'    : time_load+time_exp[n]+time_aucI+time_aucD+time_auc_IoU\n",
    "                    }\n",
    "            ##----------------------------------------------------------------------------------------##\n",
    "            data_to_csv.append(data)\n",
    "        if plot_IoU:\n",
    "            fun_plot_IoU(heatmaps,IoU,methods,path=plotsIoU_path,destroy_fig=destroy_fig,selected_ext=selected_ext,\n",
    "                         save_fig=save_fig,plot_title=plot_title, dpi=dpi)\n",
    "        if verbose_print:\n",
    "            print('-'*120)\n",
    "        df = pd.DataFrame(data_to_csv)\n",
    "        df_time = pd.DataFrame(datatime_to_csv)\n",
    "        if select_set=='full':\n",
    "            df.to_csv(file_csv, sep=',')\n",
    "            # df.to_csv(f'{results_path}/csv_exp_{DS_name_img}_{suffix}.csv', sep=',')\n",
    "    ##-------------------------------------------------------------------------------------------------\n",
    "    time_full = time.time()-st_full\n",
    "    end_time = datetime.now()\n",
    "    ##-------------------------------------------------------------------------------------------------\n",
    "    time_full_msg = f\"Time for {select_set} test is :, {time_full}\\n\"\n",
    "    duration_msg  = f\"Duration: {format(end_time - start_time)}\\n\"\n",
    "    num_images_computed = f'Computed no of images: \\t {len(np.unique(df.image))}'\n",
    "    print(duration_msg)\n",
    "    print(time_full_msg)\n",
    "    ##-------------------------------------------------------------------------------------------------\n",
    "    log_messages += str(\"=\"*120 + '\\n')\n",
    "    log_messages+= str(f'code completed {datetime.now()}\\n')\n",
    "    log_messages += duration_msg\n",
    "    log_messages += time_full_msg\n",
    "    log_messages += str(\"=\"*120 + '\\n')\n",
    "    log_messages += num_images_computed\n",
    "    \n",
    "    with open(log_file_full, \"w\") as log_file:\n",
    "        log_file.write(log_messages)\n",
    "    ##-------------------------------------------------------------------------------------------------\n",
    "    chime.success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0238c14",
   "metadata": {},
   "source": [
    "## END RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e73ff-72a7-4d72-a50a-97fdfec5fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_full_test:\n",
    "    print('Duration: \\t{}'.format(end_time - start_time))\n",
    "    print(f'Computed no of images: \\t {len(np.unique(df.image))}')\n",
    "    print('Time for Full test is :', time_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7dcef",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
